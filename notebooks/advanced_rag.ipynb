{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System with Gemini and FAISS\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Union\n",
        "\n",
        "import requests\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document, BaseRetriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "First, let's set up the configuration for our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Configuration settings\n",
        "EMBEDDING = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name=EMBEDDING)\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 80\n",
        "RETRIEVER_K = 4\n",
        "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# if not GEMINI_API_KEY:\n",
        "#     raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
        "\n",
        "# llm = GoogleGenerativeAI(\n",
        "#     api_key=GEMINI_API_KEY,\n",
        "#     model=\"gemini-2.0-flash\",\n",
        "#     verbose=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llm.model_name: gpt-3.5-turbo\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    print(\"llm.model_name:\", llm.model_name)\n",
        "except AttributeError:\n",
        "    print(\"llm.model:\", llm.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document Processing Functions\n",
        "Let's define functions to download and process PDF documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_pdf(url: str, folder: str = 'documents') -> str:\n",
        "    \"\"\"\n",
        "    Downloads PDF from given URL\n",
        "    \"\"\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    filename = os.path.basename(url.split('?')[0])\n",
        "    filepath = os.path.join(folder, filename)\n",
        "\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    return filepath\n",
        "\n",
        "def process_document(\n",
        "    documents: Union[List[str], List[Document]], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process a list of Document objects or URLs into chunks while preserving parent document relationships.\n",
        "    \"\"\"\n",
        "    if isinstance(chunk_size, str):\n",
        "        chunk_size = int(chunk_size)\n",
        "    if isinstance(chunk_overlap, str):\n",
        "        chunk_overlap = int(chunk_overlap)\n",
        "\n",
        "    if documents and isinstance(documents[0], str):\n",
        "        loaded_docs = []\n",
        "        for url in documents:\n",
        "            pdf_path = download_pdf(url)\n",
        "            pdf_docs = PyPDFLoader(pdf_path).load()\n",
        "            loaded_docs.extend(pdf_docs)\n",
        "        documents = loaded_docs\n",
        "\n",
        "    def get_filename(path):\n",
        "        if not path or path == \"unknown\":\n",
        "            return \"unknown_document\"\n",
        "        return os.path.basename(path).split('.')[0]\n",
        "\n",
        "    source_groups = {}\n",
        "    for doc in documents:\n",
        "        source = doc.metadata.get(\"source\", \"unknown\")\n",
        "        source_groups.setdefault(source, []).append(doc)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "    for source, docs in source_groups.items():\n",
        "        docs.sort(key=lambda x: x.metadata.get(\"page\", 0))\n",
        "        parent_id = get_filename(source)\n",
        "        chunks = splitter.split_documents(docs)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            page_num = chunk.metadata.get(\"page\", 0)\n",
        "            chunk_id = f\"{parent_id}_p{page_num}_c{i}\"\n",
        "            chunk.metadata.update({\n",
        "                \"parent_id\": parent_id,\n",
        "                \"parent_source\": source,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": len(chunks)\n",
        "            })\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vector Store Creation\n",
        "Now let's create a function to build our vector store from documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_and_store_documents(\n",
        "    documents: Union[List[str], List[Document]],\n",
        "    chunk_size: int,\n",
        "    chunk_overlap: int,\n",
        "    embedding_model,\n",
        "    persist_directory: str = \"faiss_index\"\n",
        ") -> FAISS:\n",
        "    chunks = process_document(documents, chunk_size, chunk_overlap)\n",
        "    if os.path.exists(persist_directory):\n",
        "        vector_store = FAISS.load_local(\n",
        "            persist_directory,\n",
        "            embedding_model,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "        existing_ids = {\n",
        "            doc.metadata.get(\"parent_id\")\n",
        "            for doc in vector_store.docstore._dict.values()\n",
        "            if \"parent_id\" in doc.metadata\n",
        "        }\n",
        "    else:\n",
        "        vector_store = None\n",
        "        existing_ids = set()\n",
        "\n",
        "    new_chunks = [c for c in chunks if c.metadata.get(\"parent_id\") not in existing_ids]\n",
        "\n",
        "    if not new_chunks:\n",
        "        print(\"No new documents to add.\")\n",
        "        return vector_store\n",
        "\n",
        "    if vector_store:\n",
        "        vector_store.add_documents(new_chunks)\n",
        "    else:\n",
        "        vector_store = FAISS.from_documents(new_chunks, embedding_model)\n",
        "\n",
        "    vector_store.save_local(persist_directory)\n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Long-Term memory storage in Vectorstore\n",
        "This part is created to store conversation with LLM as additional context to be provided during retriving data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_memory_vectorstore(embedding_model, path=\"memory_vectorstore\"):\n",
        "    if Path(path).exists():\n",
        "        return FAISS.load_local(path, embedding_model, allow_dangerous_deserialization=True)\n",
        "    \n",
        "    dim = len(embedding_model.embed_query(\"test\"))\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "    docstore = InMemoryDocstore()\n",
        "    index_to_docstore_id = {}\n",
        "    \n",
        "    return FAISS(\n",
        "        embedding_function=embedding_model,\n",
        "        index=index,\n",
        "        docstore=docstore,\n",
        "        index_to_docstore_id=index_to_docstore_id\n",
        "    )\n",
        "\n",
        "def store_to_memory_vectorstore(question, answer, vectorstore):\n",
        "    content = f\"Q: {question}\\nA: {answer}\"\n",
        "    doc = Document(page_content=content, metadata={\"type\": \"chat_memory\"})\n",
        "    vectorstore.add_documents([doc])\n",
        "    vectorstore.save_local(\"memory_vectorstore\")\n",
        "    \n",
        "def get_relevant_memory(query, vectorstore, k=3):\n",
        "    try:\n",
        "        memory_docs = vectorstore.similarity_search(query, k=k)\n",
        "        return \"\\n---\\n\".join([doc.page_content for doc in memory_docs])\n",
        "    except (IndexError, ValueError) as e:\n",
        "        print(f\"Skipping memory retrieval (reason: {str(e)})\")\n",
        "        return \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_parent_document_llm_reranker(vectorstore, top_k_chunks=20, top_k_parents=4):\n",
        "    class LLMRerankerRetriever(BaseRetriever):\n",
        "        def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "            relevant_chunks_with_scores = vectorstore.similarity_search_with_score(query, k=top_k_chunks)\n",
        "            chunks = [doc for doc, _ in relevant_chunks_with_scores]\n",
        "            scores = [score for _, score in relevant_chunks_with_scores]\n",
        "\n",
        "            parent_docs = {}\n",
        "            for chunk, score in zip(chunks, scores):\n",
        "                parent_id = chunk.metadata.get(\"parent_id\") or chunk.metadata.get(\"doc_id\", f\"doc_{len(parent_docs)}\")\n",
        "                if parent_id not in parent_docs:\n",
        "                    parent_docs[parent_id] = {\n",
        "                        \"chunks\": [],\n",
        "                        \"scores\": [],\n",
        "                        \"source\": chunk.metadata.get(\"parent_source\", \"unknown\")\n",
        "                    }\n",
        "                parent_docs[parent_id][\"chunks\"].append(chunk)\n",
        "                parent_docs[parent_id][\"scores\"].append(score)\n",
        "\n",
        "            parent_list = []\n",
        "            for parent_id, parent in parent_docs.items():\n",
        "                parent[\"chunks\"].sort(key=lambda x: (x.metadata.get(\"page\", 0), x.metadata.get(\"chunk_index\", 0)))\n",
        "                full_text = \"\\n\".join([chunk.page_content for chunk in parent[\"chunks\"]])\n",
        "\n",
        "                try:\n",
        "                    rerank_score = float(cross_encoder.predict([(query, full_text)])[0])\n",
        "                except Exception:\n",
        "                    rerank_score = 0.0\n",
        "\n",
        "                for c in parent[\"chunks\"]:\n",
        "                    c.metadata[\"rerank_score\"] = rerank_score\n",
        "\n",
        "                parent_list.append({\n",
        "                    \"id\": parent_id,\n",
        "                    \"chunks\": parent[\"chunks\"],\n",
        "                    \"rerank_score\": rerank_score,\n",
        "                    \"source\": parent[\"source\"]\n",
        "                })\n",
        "\n",
        "            parent_list.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "            top_docs = []\n",
        "            for parent in parent_list[:top_k_parents]:\n",
        "                top_docs.extend(parent[\"chunks\"])\n",
        "\n",
        "            return top_docs\n",
        "\n",
        "        async def _aget_relevant_documents(self, query: str):\n",
        "            raise NotImplementedError(\"Async version not implemented.\")\n",
        "\n",
        "    return LLMRerankerRetriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Different types of prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_prompt(prompt_type: str, role: str = \"Ai Assistant\") -> PromptTemplate:\n",
        "    if prompt_type == \"zero_shot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Be clear and concise.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"explain_like_5\":\n",
        "        template = \"\"\"Use the following pieces of context to answer the question. Explain like you are talking to a 5-year-old. \n",
        "        If the question is not related to the context, say \"I don't know\". If you don't know the answer, just say that you don't know.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Provide a clear and concise answer.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"cot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Think step-by-step and explain your reasoning.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Let's think step by step:\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"elaborate\":\n",
        "        template = \"\"\"Use the following context to answer the question in a detailed, formal tone. If you can't answer, say \"I don't know\".\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Detailed answer:\"\"\"\n",
        "    elif prompt_type == \"meta\":\n",
        "        template = \"\"\"You are an AI assistant tasked with answering the question using the provided context. \n",
        "        First, generate an optimal prompt that would help an LLM perform this task effectively.\n",
        "        Then, respond to that prompt yourself to complete the task.\n",
        "        Reflect on your reasoning process as you answer. Clearly state what you know, what you are assuming, and how confident you are.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer (include reasoning, assumptions, and confidence level):\"\"\"\n",
        "    elif prompt_type == \"role\":\n",
        "        template = \"\"\"You are acting as {role}. Use the following context to answer the question appropriately for your role.\n",
        "        If the question is not related to the context, say \"I don't know\". If you're unsure of the answer, acknowledge that.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        As a {role}, your answer:\"\"\"\n",
        "    elif prompt_type == \"react\":\n",
        "        template = \"\"\"You are an intelligent assistant that reasons step-by-step and can use external context to answer questions.\n",
        "\n",
        "        Use the following format:\n",
        "\n",
        "        Question: {question}\n",
        "        Thought: Think about what you need to find.\n",
        "        Action: Look up relevant information in the context.\n",
        "        Observation: Summarize what the context says.\n",
        "        Thought: Reflect on how the observation answers the question.\n",
        "        Final Answer: Give a complete and clear answer.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Now follow the steps to answer:\n",
        "        \"\"\"\n",
        "    elif prompt_type == \"verify\":\n",
        "        template = \"\"\"\n",
        "        Given the context and the answer, verify if the answer is fully supported. \n",
        "        Respond with YES or NO, then explain briefly.\n",
        "        \n",
        "        Context:\n",
        "        {context}\n",
        "        \n",
        "        Answer:\n",
        "        {answer}\n",
        "        \n",
        "        Is this answer verifiable?\n",
        "        \"\"\"\n",
        "    elif prompt_type == \"rewrite\":\n",
        "        template = \"\"\"\n",
        "        You are an AI assistant helping to optimize search queries for a Retrieval-Augmented Generation (RAG) system.\n",
        "        Your task is to rewrite the user's original query to make it more specific, context-rich, and focused,\n",
        "        so it improves the chances of retrieving the most relevant documents.\n",
        "\n",
        "        Original Query: {question}\n",
        "\n",
        "        Rewritten Query:\n",
        "        \"\"\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n",
        "\n",
        "    return PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt rewriting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rewrite_query(original_query, llm):\n",
        "    prompt = get_prompt(\"rewrite\")\n",
        "    rewriter = prompt | llm\n",
        "    response = rewriter.invoke(original_query)\n",
        "    print(\"Rewritten query:\", response)\n",
        "    content = response.content if hasattr(response, \"content\") else str(response)\n",
        "    print(\"Rewritten content:\", content)\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG Chain Creation\n",
        "Let's create our RAG chain with the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_chain(\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt_type: str = \"zero_shot\",\n",
        "    use_memory: bool = False,\n",
        "    use_reranking: bool = False,\n",
        "    rewrite: bool = False,\n",
        "    retriever_k: int = 4,\n",
        "    top_k_chunks: int = 20,\n",
        "    additional_prompt_instruction: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a RAG chain with optional memory, reranking, query rewriting,\n",
        "    and additional prompt instruction (e.g., 'limit to 100 words').\n",
        "    \"\"\"\n",
        "    prompt = get_prompt(prompt_type)\n",
        "\n",
        "    if additional_prompt_instruction:\n",
        "        prompt.template += f\"\\n\\nInstruction: {additional_prompt_instruction}\"\n",
        "\n",
        "    memory = (\n",
        "        ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            input_key=\"question\",\n",
        "            output_key=\"answer\",\n",
        "            return_messages=True\n",
        "        ) if use_memory else None\n",
        "    )\n",
        "\n",
        "    retriever = (\n",
        "        create_parent_document_llm_reranker(\n",
        "            vectorstore=vectorstore,\n",
        "            top_k_chunks=top_k_chunks,\n",
        "            top_k_parents=retriever_k\n",
        "        ) if use_reranking else\n",
        "        vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": retriever_k}\n",
        "        )\n",
        "    )\n",
        "\n",
        "    base_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    if rewrite:\n",
        "        def chain_with_rewriting(inputs):\n",
        "            inputs[\"question\"] = rewrite_query(inputs[\"question\"], llm)\n",
        "            return base_chain.invoke(inputs)\n",
        "        return chain_with_rewriting, memory\n",
        "\n",
        "    return base_chain, memory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_result_summary(result):\n",
        "    print(f\"\\nğŸ§  Answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "    print(\"ğŸ” Retrieved Documents:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "        print(f\"\\nğŸ“„ Chunk #{i}\")\n",
        "        print(f\"ğŸ“š Source: {doc.metadata.get('parent_source', 'unknown')}\")\n",
        "        print(f\"ğŸ“„ Page: {doc.metadata.get('page', 'unknown')}\")\n",
        "        print(f\"ğŸ·ï¸ Rerank Score: {doc.metadata.get('rerank_score', 'N/A')}\")\n",
        "        #print(\"ğŸ“ Excerpt:\")\n",
        "        #print(doc.page_content.strip()[:500] + (\"...\" if len(doc.page_content) > 500 else \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_rag(\n",
        "    new_docs: List[str] = None,\n",
        "    persist_directory: str = \"./faiss_index\",\n",
        "    chunk_size: str = \"800\",\n",
        "    chunk_overlap: str = \"80\",\n",
        "    model_choice: str = \"gemini\"  # or \"gpt\"\n",
        "):\n",
        "    load_dotenv()\n",
        "    base_docs = [\n",
        "        'https://assets.pokemon.com/assets/cms2/pdf/trading-card-game/rulebook/sm7_rulebook_en.pdf',\n",
        "        'https://media.wizards.com/images/magic/tcg/resources/rules/MagicCompRules_21031101.pdf',\n",
        "        'https://cdn.1j1ju.com/medias/d3/22/83-monopoly-rulebook.pdf',\n",
        "        'https://fgbradleys.com/wp-content/uploads/rules/Monopoly_Rules.pdf?srsltid=AfmBOorDaiGKyaEWIQFd-au0rl8-tKoqedlzy_6r4EETpj_ZMIUYsNMQ'\n",
        "    ]\n",
        "\n",
        "    all_docs = base_docs + (new_docs or [])\n",
        "\n",
        "    faiss_db = process_and_store_documents(\n",
        "        documents=all_docs,\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        embedding_model=EMBEDDING_MODEL,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "\n",
        "    if model_choice == \"gpt\":\n",
        "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if not openai_key:\n",
        "            raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
        "        llm = ChatOpenAI(api_key=openai_key, model=\"gpt-3.5-turbo\", temperature=0.0)\n",
        "\n",
        "    elif model_choice == \"gemini\":\n",
        "        gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
        "        if not gemini_key:\n",
        "            raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
        "        llm = GoogleGenerativeAI(api_key=gemini_key, model=\"gemini-2.0-flash\", verbose=False)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model_choice. Use 'gpt' or 'gemini'.\")\n",
        "\n",
        "    return faiss_db, llm\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prepare documents and vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No new documents to add.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    faiss_db, llm = initialize_rag()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    rag_chain, rag_memory = create_rag_chain(\n",
        "        vectorstore=faiss_db,\n",
        "        llm=llm,\n",
        "        prompt_type=\"react\",\n",
        "        use_memory=True,\n",
        "        use_reranking=True,\n",
        "        rewrite=True,\n",
        "        additional_prompt_instruction=\"provide only final answer\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Testing with Sample Queries\n",
        "Let's test our RAG system with some sample queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ğŸ” Query: what is EX card?\n",
            "Rewritten query: Explain the concept of a PokÃ©mon EX card, including its key characteristics, gameplay mechanics, and differences from regular PokÃ©mon cards.\n",
            "\n",
            "ğŸ§  Answer:\n",
            "A PokÃ©mon EX card is a powerful version of a PokÃ©mon with higher HP and stronger attacks than regular PokÃ©mon cards. The \"EX\" is part of the PokÃ©mon's name, allowing you to have up to 4 of the regular version and 4 of the EX version in your deck. The major drawback of PokÃ©mon-EX is that when one is Knocked Out, your opponent takes 2 Prize cards instead of 1. They play like any other PokÃ©mon card otherwise and can come in regular and full-art rare Ultra versions.\n",
            "\n",
            "ğŸ” Retrieved Documents:\n",
            "\n",
            "ğŸ“„ Chunk #1\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 0\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #2\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 2\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #3\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 2\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #4\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 4\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #5\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 5\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #6\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 10\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #7\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 21\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #8\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 25\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #9\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 27\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #10\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 28\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #11\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 31\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #12\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 31\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #13\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 31\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #14\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 32\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #15\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 32\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #16\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 32\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #17\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 32\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #18\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 32\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #19\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 33\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n",
            "\n",
            "ğŸ“„ Chunk #20\n",
            "ğŸ“š Source: documents\\sm7_rulebook_en.pdf\n",
            "ğŸ“„ Page: 33\n",
            "ğŸ·ï¸ Rerank Score: -0.997477650642395\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    test_queries = [\n",
        "        \"what is EX card?\"\n",
        "    ]\n",
        "    \n",
        "    for query in test_queries:\n",
        "        print(f\"\\nğŸ” Query: {query}\")\n",
        "        result = rag_chain({\"question\": query})\n",
        "        print_result_summary(result)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
