{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System with Gemini and FAISS\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List, Union\n",
        "from dotenv import load_dotenv\n",
        "import requests\n",
        "from langchain.schema import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "First, let's set up the configuration for our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Configuration settings\n",
        "EMBEDDING = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name=EMBEDDING)\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 80\n",
        "RETRIEVER_K = 4\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
        "\n",
        "llm = GoogleGenerativeAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    verbose=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document Processing Functions\n",
        "Let's define functions to download and process PDF documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_pdf(url: str, folder: str = 'documents') -> str:\n",
        "    \"\"\"\n",
        "    Downloads PDF from given URL\n",
        "    \"\"\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    filename = os.path.basename(url.split('?')[0])\n",
        "    filepath = os.path.join(folder, filename)\n",
        "\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    return filepath\n",
        "\n",
        "def process_document(\n",
        "    documents: Union[List[str], List[Document]], \n",
        "    chunk_size: int, \n",
        "    chunk_overlap: int\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process a list of Document objects or URLs into chunks while preserving parent document relationships.\n",
        "    \"\"\"\n",
        "    if isinstance(chunk_size, str):\n",
        "        chunk_size = int(chunk_size)\n",
        "    if isinstance(chunk_overlap, str):\n",
        "        chunk_overlap = int(chunk_overlap)\n",
        "\n",
        "    if documents and isinstance(documents[0], str):\n",
        "        loaded_docs = []\n",
        "        for url in documents:\n",
        "            pdf_path = download_pdf(url)\n",
        "            pdf_docs = PyPDFLoader(pdf_path).load()\n",
        "            loaded_docs.extend(pdf_docs)\n",
        "        documents = loaded_docs\n",
        "\n",
        "    def get_filename(path):\n",
        "        if not path or path == \"unknown\":\n",
        "            return \"unknown_document\"\n",
        "        return os.path.basename(path).split('.')[0]\n",
        "\n",
        "    source_groups = {}\n",
        "    for doc in documents:\n",
        "        source = doc.metadata.get(\"source\", \"unknown\")\n",
        "        source_groups.setdefault(source, []).append(doc)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "    for source, docs in source_groups.items():\n",
        "        docs.sort(key=lambda x: x.metadata.get(\"page\", 0))\n",
        "        parent_id = get_filename(source)\n",
        "        chunks = splitter.split_documents(docs)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            page_num = chunk.metadata.get(\"page\", 0)\n",
        "            chunk_id = f\"{parent_id}_p{page_num}_c{i}\"\n",
        "            chunk.metadata.update({\n",
        "                \"parent_id\": parent_id,\n",
        "                \"parent_source\": source,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": len(chunks)\n",
        "            })\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vector Store Creation\n",
        "Now let's create a function to build our vector store from documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "def process_and_store_documents(documents: List[str], \n",
        "                              chunk_size: int, \n",
        "                              chunk_overlap: int,\n",
        "                              embedding_model = EMBEDDING_MODEL,\n",
        "                              persist_directory: str = None) -> FAISS:\n",
        "    \"\"\"\n",
        "    Process documents into chunks and store them in a FAISS vector database\n",
        "    while preserving parent document relationships.\n",
        "    \"\"\"\n",
        "    # First process the documents into chunks with parent metadata\n",
        "    chunks = process_document(documents, chunk_size, chunk_overlap)\n",
        "    \n",
        "    vector_store = FAISS.from_documents(chunks, embedding_model)\n",
        "    \n",
        "    # Optionally persist to disk\n",
        "    if persist_directory:\n",
        "        vector_store.save_local(persist_directory)\n",
        "    \n",
        "    print(f\"Added {len(chunks)} chunks from {len(set([c.metadata['parent_id'] for c in chunks]))} parent documents to FAISS vector store\")\n",
        "    \n",
        "    return vector_store\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Long-Term memory storage in Vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import faiss\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "def create_memory_vectorstore(embedding_model, path=\"memory_vectorstore\"):\n",
        "    if Path(path).exists():\n",
        "        return FAISS.load_local(path, embedding_model, allow_dangerous_deserialization=True)\n",
        "    \n",
        "    dim = len(embedding_model.embed_query(\"test\"))\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    docstore = InMemoryDocstore()\n",
        "    index_to_docstore_id = {}\n",
        "    \n",
        "    return FAISS(\n",
        "        embedding_function=embedding_model,\n",
        "        index=index,\n",
        "        docstore=docstore,\n",
        "        index_to_docstore_id=index_to_docstore_id\n",
        "    )\n",
        "\n",
        "def store_to_memory_vectorstore(question, answer, vectorstore, embedding_model):\n",
        "    content = f\"Q: {question}\\nA: {answer}\"\n",
        "    doc = Document(page_content=content, metadata={\"type\": \"chat_memory\"})\n",
        "    vectorstore.add_documents([doc])\n",
        "    vectorstore.save_local(\"memory_vectorstore\")\n",
        "    \n",
        "def get_relevant_memory(query, vectorstore, k=3):\n",
        "    try:\n",
        "        memory_docs = vectorstore.similarity_search(query, k=k)\n",
        "        return \"\\n---\\n\".join([doc.page_content for doc in memory_docs])\n",
        "    except (IndexError, ValueError) as e:\n",
        "        # Common errors for empty vectorstore or dimension mismatch\n",
        "        print(f\"Skipping memory retrieval (reason: {str(e)})\")\n",
        "        return \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
        "from sentence_transformers import CrossEncoder\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "\n",
        "def create_parent_document_reranker(vectorstore, reranker_model, top_k_chunks=20, top_k_parents=4):\n",
        "    class FunctionalRerankerRetriever(BaseRetriever):\n",
        "        def get_relevant_documents(self, query: str) -> list[Document]:\n",
        "            relevant_chunks_with_scores = vectorstore.similarity_search_with_score(query, k=top_k_chunks)\n",
        "            chunks = [doc for doc, _ in relevant_chunks_with_scores]\n",
        "            scores = [score for _, score in relevant_chunks_with_scores]\n",
        "\n",
        "            parent_docs = {}\n",
        "            for chunk, score in zip(chunks, scores):\n",
        "                parent_id = chunk.metadata.get(\"parent_id\") or chunk.metadata.get(\"doc_id\", f\"doc_{len(parent_docs)}\")\n",
        "                if parent_id not in parent_docs:\n",
        "                    parent_docs[parent_id] = {\n",
        "                        \"chunks\": [],\n",
        "                        \"scores\": [],\n",
        "                        \"source\": chunk.metadata.get(\"parent_source\", \"unknown\")\n",
        "                    }\n",
        "                parent_docs[parent_id][\"chunks\"].append(chunk)\n",
        "                parent_docs[parent_id][\"scores\"].append(score)\n",
        "\n",
        "            parent_list = []\n",
        "            for parent_id, parent in parent_docs.items():\n",
        "                parent[\"chunks\"].sort(key=lambda x: (x.metadata.get(\"page\", 0), x.metadata.get(\"chunk_index\", 0)))\n",
        "                full_text = \"\\n\".join([chunk.page_content for chunk in parent[\"chunks\"]])\n",
        "                rerank_score = reranker_model.predict([(query, full_text)])[0]\n",
        "\n",
        "                for c in parent[\"chunks\"]:\n",
        "                    c.metadata[\"rerank_score\"] = rerank_score\n",
        "\n",
        "                parent_list.append({\n",
        "                    \"id\": parent_id,\n",
        "                    \"chunks\": parent[\"chunks\"],\n",
        "                    \"rerank_score\": rerank_score,\n",
        "                    \"source\": parent[\"source\"]\n",
        "                })\n",
        "\n",
        "            parent_list.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "            top_docs = []\n",
        "            for parent in parent_list[:top_k_parents]:\n",
        "                top_docs.extend(parent[\"chunks\"])\n",
        "\n",
        "            return top_docs\n",
        "\n",
        "        async def aget_relevant_documents(self, query: str):\n",
        "            raise NotImplementedError(\"Async version not implemented.\")\n",
        "\n",
        "    return FunctionalRerankerRetriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Different types of prompts\n",
        "Now let's create a function to build our vector store from documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_prompt(prompt_type: str, role: str = \"Ai Assistant\") -> PromptTemplate:\n",
        "    if prompt_type == \"zero_shot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Be clear and concise.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"explain_like_5\":\n",
        "        template = \"\"\"Use the following pieces of context to answer the question. Explain like you are talking to a 5-year-old. \n",
        "        If the question is not related to the context, say \"I don't know\". If you don't know the answer, just say that you don't know.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Provide a clear and concise answer.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"cot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Think step-by-step and explain your reasoning.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Let's think step by step:\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"elaborate\":\n",
        "        template = \"\"\"Use the following context to answer the question in a detailed, formal tone. If you can't answer, say \"I don't know\".\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Detailed answer:\"\"\"\n",
        "    elif prompt_type == \"meta\":\n",
        "        template = \"\"\"You are an AI assistant tasked with answering the question using the provided context. \n",
        "        First, generate an optimal prompt that would help an LLM perform this task effectively.\n",
        "        Then, respond to that prompt yourself to complete the task.\n",
        "        Reflect on your reasoning process as you answer. Clearly state what you know, what you are assuming, and how confident you are.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer (include reasoning, assumptions, and confidence level):\"\"\"\n",
        "    elif prompt_type == \"role\":\n",
        "        template = \"\"\"You are acting as {role}. Use the following context to answer the question appropriately for your role.\n",
        "        If the question is not related to the context, say \"I don't know\". If you're unsure of the answer, acknowledge that.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        As a {role}, your answer:\"\"\"\n",
        "    elif prompt_type == \"react\":\n",
        "        template = \"\"\"You are an intelligent assistant that reasons step-by-step and can use external context to answer questions.\n",
        "\n",
        "        Use the following format:\n",
        "\n",
        "        Question: {question}\n",
        "        Thought: Think about what you need to find.\n",
        "        Action: Look up relevant information in the context.\n",
        "        Observation: Summarize what the context says.\n",
        "        Thought: Reflect on how the observation answers the question.\n",
        "        Final Answer: Give a complete and clear answer.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Now follow the steps to answer:\n",
        "        \"\"\"\n",
        "    elif prompt_type == \"verify\":\n",
        "        template = \"\"\"\n",
        "        Given the context and the answer, verify if the answer is fully supported. \n",
        "        Respond with YES or NO, then explain briefly.\n",
        "        \n",
        "        Context:\n",
        "        {context}\n",
        "        \n",
        "        Answer:\n",
        "        {answer}\n",
        "        \n",
        "        Is this answer verifiable?\n",
        "        \"\"\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n",
        "\n",
        "    return PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. RAG Chain Creation\n",
        "Let's create our RAG chain with the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.schema import BaseRetriever\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "\n",
        "def create_rag_chain(\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt_type: str = \"zero_shot\",\n",
        "    use_memory: bool = False,\n",
        "    use_reranking: bool = False,\n",
        "    reranker_model=None,\n",
        "    retriever_k: int = 4,\n",
        "    top_k_chunks: int = 20,\n",
        "    rewrite_prompt: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a RAG chain with optional memory, reranking, and query rewriting.\n",
        "    \"\"\"\n",
        "    prompt = get_prompt(prompt_type)\n",
        "\n",
        "    memory = (\n",
        "        ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            input_key=\"question\",\n",
        "            output_key=\"answer\",\n",
        "            return_messages=True\n",
        "        )\n",
        "        if use_memory else None\n",
        "    )\n",
        "\n",
        "    if use_reranking:\n",
        "        reranker_model = reranker_model or CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "        retriever = create_parent_document_reranker(\n",
        "            vectorstore=vectorstore,\n",
        "            reranker_model=reranker_model,\n",
        "            top_k_chunks=top_k_chunks,\n",
        "            top_k_parents=retriever_k\n",
        "        )\n",
        "    else:\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": retriever_k}\n",
        "        )\n",
        "\n",
        "    base_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    if rewrite_prompt:\n",
        "        rewrite_template = (\n",
        "            \"Given the chat history:\\n\\n{chat_history}\\n\\n\"\n",
        "            \"And the new question:\\n{question}\\n\\n\"\n",
        "            \"Rewrite the question to be standalone, give only one best example:\"\n",
        "            if use_memory else\n",
        "            \"Given the new question: {question}\\n\\nRewrite the question to be standalone, give only one best example:\"\n",
        "        )\n",
        "\n",
        "        rewrite_prompt_obj = PromptTemplate.from_template(rewrite_template)\n",
        "        rewriter_chain = LLMChain(llm=llm, prompt=rewrite_prompt_obj)\n",
        "\n",
        "        def wrapped_chain(inputs):\n",
        "            rewrite_input = {\"question\": inputs[\"question\"]}\n",
        "\n",
        "            chat_history_text = \"\"\n",
        "            if use_memory and memory is not None:\n",
        "                chat_history = memory.load_memory_variables({}).get(\"chat_history\", [])\n",
        "                chat_history_text = \"\\n\".join(\n",
        "                    [f\"{msg.type.title()}: {msg.content}\" for msg in chat_history]\n",
        "                )\n",
        "                rewrite_input[\"chat_history\"] = chat_history_text\n",
        "\n",
        "            standalone_question = rewriter_chain.run(rewrite_input)\n",
        "\n",
        "            # Explicitly passing 'chat_history' to base_chain fixes the error\n",
        "            result = base_chain({\n",
        "                \"question\": standalone_question,\n",
        "                \"chat_history\": chat_history_text if use_memory else []\n",
        "            })\n",
        "\n",
        "            result.update({\n",
        "                \"standalone_question\": standalone_question,\n",
        "                \"original_question\": inputs[\"question\"]\n",
        "            })\n",
        "            return result\n",
        "\n",
        "        return wrapped_chain, memory\n",
        "\n",
        "    return base_chain, memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_result_summary(result):\n",
        "    print(f\"\\n🧠 Answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "    print(\"🔎 Retrieved Documents:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "        print(f\"\\n📄 Chunk #{i}\")\n",
        "        print(f\"📚 Source: {doc.metadata.get('parent_source', 'unknown')}\")\n",
        "        print(f\"📄 Page: {doc.metadata.get('page', 'unknown')}\")\n",
        "        print(f\"🏷️ Rerank Score: {doc.metadata.get('rerank_score', 'N/A')}\")\n",
        "        print(\"📝 Excerpt:\")\n",
        "        print(doc.page_content.strip()[:500] + (\"...\" if len(doc.page_content) > 500 else \"\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Building the Complete RAG System\n",
        "Now let's put everything together to build our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 1074 chunks from 4 parent documents to FAISS vector store\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are trying to use a model that was created with Sentence Transformers version 4.1.0.dev0, but you're currently using version 4.0.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
          ]
        }
      ],
      "source": [
        "# Prepare documents\n",
        "docs = ['https://assets.pokemon.com/assets/cms2/pdf/trading-card-game/rulebook/sm7_rulebook_en.pdf','https://media.wizards.com/images/magic/tcg/resources/rules/MagicCompRules_21031101.pdf','https://cdn.1j1ju.com/medias/d3/22/83-monopoly-rulebook.pdf','https://fgbradleys.com/wp-content/uploads/rules/Monopoly_Rules.pdf?srsltid=AfmBOorDaiGKyaEWIQFd-au0rl8-tKoqedlzy_6r4EETpj_ZMIUYsNMQ']\n",
        "process_and_store_documents(docs,'800','80', persist_directory=\"./faiss_index\")\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "faiss_db = FAISS.load_local(\"./faiss_index\", EMBEDDING_MODEL, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_9648\\1209290737.py:67: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  rewriter_chain = LLMChain(llm=llm, prompt=rewrite_prompt_obj)\n"
          ]
        }
      ],
      "source": [
        "rag_chain, rag_memory = create_rag_chain(\n",
        "    vectorstore=faiss_db,\n",
        "    llm=llm,\n",
        "    prompt_type=\"zero_shot\",\n",
        "    use_memory=False,\n",
        "    reranker_model=reranker,\n",
        "    retriever_k=3,\n",
        "    use_reranking=False,\n",
        "    top_k_chunks=10,\n",
        "    rewrite_prompt=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Testing with Sample Queries\n",
        "Let's test our RAG system with some sample queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_9648\\1209290737.py:80: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  standalone_question = rewriter_chain.run(rewrite_input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Query: what is EX card?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_9648\\1209290737.py:83: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result = base_chain({\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Updated Query: \n",
            "\n",
            "What is an EX card in the context of Pokemon Trading Card Game?\n",
            "\n",
            "Answer:\n",
            "An EX card is a special type of Pokemon card in the Pokemon Trading Card Game. It was introduced in the EX Series, which began in 2001. EX cards feature powerful Pokemon with higher stats and abilities compared to regular cards. For example, Charizard EX from the Charizard EX Holofoil card (EX Holo: Charizard #112/112) is an EX card. It has a higher HP and attack power than the regular Charizard card. Additionally, it has special abilities like \"Double Dragon Dance,\" which allows it to attack twice in one turn.\n",
            "\n",
            "🧠 Answer:\n",
            " (Alternative)\n",
            "\n",
            "An EX card is a type of Pokemon card in the Pokemon Trading Card Game that represents a more powerful version of a Pokemon. These cards were introduced in the EX Series and have distinct characteristics, such as a larger size, a special symbol (EX), and unique abilities. When an EX card is Knocked Out, the opponent takes two Prize cards instead of one. Examples of EX cards include Yveltal-EX and Charizard-EX.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 27\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "28\n",
            "Pokémon T rading Card Game Rules\n",
            "APPENDIX J: POKÉMON-EX\n",
            "Pokémon-EX are powerful Pokémon that show off a Pokémon with more HP and stronger attacks than regular \n",
            "Pokémon, but there are risks to playing these powered-up Pokémon!\n",
            "SPECIAL RULES FOR POKÉMON -EX\n",
            "The EX is part of a Pokémon-EX’s name. Thus Yveltal and Yveltal-EX \n",
            "have different names, so you can have up to 4 of each in your deck \n",
            "if you wish.\n",
            "When one of your Pokémon-EX is Knocked Out, your opponent \n",
            "takes 2 Prize cards.\n",
            "Apart from t...\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 32\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "POKÉMON:  The colorful characters that fight for you in the \n",
            "Pokémon Trading Card Game. They are represented in the game \n",
            "by Basic Pokémon and Evolution cards.\n",
            "POKÉMON-EX : Pokémon-EX are a stronger form of Pokémon \n",
            "with a special drawback: when your Pokémon-EX is Knocked Out, \n",
            "your opponent takes two Prize cards instead of one.\n",
            "POKÉMON-GX : Pokémon-GX are a stronger form of \n",
            "Pokémon with a special drawback: when your Pokémon-GX is \n",
            "Knocked Out, your opponent takes two Prize cards instead of one...\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 32\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "of Pokémon-EX with an additional drawback: when one  \n",
            "of your Pokémon becomes a Mega Evolution Pokémon, your  \n",
            "turn ends.\n",
            "OWNER: A Pokémon with a Trainer’s name in its title, such as \n",
            "Brock’s Sandshrew or Team Rocket’s Meowth. Cards with , , , \n",
            ", or  do NOT count as “Owner” cards.\n",
            "POISON MARKER:  Object put on a Pokémon to remind  \n",
            "you it is Poisoned. Remove the marker if the Pokémon is Benched \n",
            "or evolved.\n",
            "POKÉ-BODY: An effect that is active as soon as that Pokémon \n",
            "is in play and lasts until t...\n",
            "\n",
            "🔍 Query: what is haste?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Updated Query: \n",
            "\n",
            "What is haste, and how does it affect a character in a role-playing game?\n",
            "\n",
            "Example answer:\n",
            "\n",
            "Haste is a status effect in many role-playing games that increases a character's speed and often grants additional benefits, such as increased attack speed or the ability to perform multiple actions in a single turn. For instance, in the game Dungeons & Dragons, a character under the effect of a haste spell can perform twice as many actions in a turn as they normally would. However, the use of haste often comes with drawbacks, such as increased risk of error or reduced effectiveness of certain abilities.\n",
            "\n",
            "🧠 Answer:\n",
            "\n",
            "\n",
            "Haste is a static ability in the Magic: The Gathering card game that allows a creature to attack even if it hasn't been controlled by its controller continuously since their most recent turn began. It doesn't have any direct impact on a character in a role-playing game, but it can be simulated by granting a character an extra action or allowing them to take actions out of turn. However, it's important to note that the rules for haste in Magic: The Gathering and in role-playing games may differ significantly.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 102\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "702.8b Multiple instances of flash on the same object are redundant. \n",
            " \n",
            "702.9. Flying \n",
            " \n",
            "702.9a Flying is an evasion ability. \n",
            " \n",
            "702.9b A creature with flying can’t be blocked except by creatures with flying and/or reach. A \n",
            "creature with flying can block a creature with or without flying. (See rule 509, “Declare \n",
            "Blockers Step,” and rule 702.17, “Reach.”) \n",
            " \n",
            "702.9c Multiple instances of flying on the same creature are redundant. \n",
            " \n",
            "702.10. Haste \n",
            " \n",
            "702.10a Haste is a static ability. \n",
            " \n",
            "702.10b ...\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 172\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "Defending Player \n",
            "The player who can be attacked, and whose planeswalkers can be attacked, during the combat phase. See \n",
            "rule 506.2. In certain multiplayer games, there may be more than one defending player; see rule 802, \n",
            "“Attack Multiple Players Option,” and rule 810.7. \n",
            " \n",
            "Defending Team \n",
            "The team who can be attacked, and whose planeswalkers can be attacked, during the combat phase of a \n",
            "Two-Headed Giant game. See rule 810.7. \n",
            " \n",
            "Delayed Triggered Ability \n",
            "An ability created by effects generate...\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 116\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "this card by putting it onto the stack from your hand, you may pay [cost] and exile it with N \n",
            "time counters on it. This action doesn’t use the stack,” and “At the beginning of your upkeep, if \n",
            "this card is suspended, remove a time counter from it,” and “When the last time counter is \n",
            "removed from this card, if it’s exiled, play it without paying its mana cost if able. If you can’t, it \n",
            "remains exiled. If you cast a creature spell this way, it gains haste until you lose control of the \n",
            "spell or ...\n",
            "\n",
            "🔍 Query: how to build a hotel?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Updated Query: \n",
            "\n",
            "What are the steps to build a luxury hotel from the ground up?\n",
            "\n",
            "1. Location Selection: Choose a prime location with easy access to transportation, tourist attractions, and business centers.\n",
            "2. Market Research: Conduct thorough market research to understand the demand for hotels in the area, competition, and target audience.\n",
            "3. Business Plan: Develop a comprehensive business plan, including financial projections, marketing strategy, and operational structure.\n",
            "4. Design and Architecture: Hire a reputable architectural firm to design the hotel, ensuring it aligns with local building codes and regulations.\n",
            "5. Permits and Approvals: Obtain all necessary permits and approvals from local authorities, including zoning permits, building permits, and environmental permits.\n",
            "6. Construction: Hire a reputable construction company to build the hotel, ensuring they have experience in constructing hotels and can deliver the project on time and within budget.\n",
            "7. Furnishing and Decor: Source high-quality furniture, fixtures, and decor to create a luxurious and inviting atmosphere for guests.\n",
            "8. Staffing: Recruit and train a team of skilled and friendly staff to provide excellent customer service and ensure a smooth operation.\n",
            "9. Marketing and Branding: Develop a strong marketing and branding strategy to attract guests and establish a reputation as a luxury hotel.\n",
            "10. Grand Opening: Plan and execute a grand opening event to generate buzz and excitement around the hotel, inviting local media, influencers, and potential guests.\n",
            "\n",
            "🧠 Answer:\n",
            "\n",
            "\n",
            "To build a luxury hotel from the ground up in Monopoly, follow these steps:\n",
            "\n",
            "1. Acquire a complete color-group of properties.\n",
            "2. Build four houses on each property of the color-group.\n",
            "3. Once you have four houses on each property, you may buy a hotel from the Bank and erect it on any property of the color-group.\n",
            "4. Return the four houses from that property to the Bank and pay the price for the hotel as shown on the Title Deed card.\n",
            "5. Ensure there are houses and hotels available for sale in the Bank before proceeding.\n",
            "6. If necessary, participate in an auction to purchase the hotel if there are multiple players vying for the same property.\n",
            "7. Sell back any unnecessary houses or hotels to the Bank at any time for one-half the price paid for them.\n",
            "8. Remember that only one hotel may be erected on any one property.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 2\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "HOTELS...When a player has four houses on each property of a complete color-group, he may buy a hotel from the Bank and erect it on \n",
            "any property of that color-group. He returns the four houses from that property to the Bank and pays the price for the hotel as shown on \n",
            "the Title Deed card. Only one hotel may be erected on any one property.  \n",
            " \n",
            "BUILDING SHORTAGE...When the Bank has no houses to sell, players wishing to build must wait for some player to turn back or to \n",
            "sell his houses to the Ba...\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "houses back to the Bank (see SELLING PROPERTY).\n",
            "HOTELS… When a player has four houses on each property of a\n",
            "complete color-group, he/she may buy a hotel from the Bank and\n",
            "erect it on any property of the color-group. He/she returns the four\n",
            "houses from that property to the Bank and pays the price for the hotel\n",
            "as shown on the Title Deed card. Only one hotel may be erected on\n",
            "any one property.\n",
            "BUILDING SHORTAGES… When the Bank has no houses to sell,\n",
            "players wishing to build must wait for some play...\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "auction to the highest bidder.\n",
            "SELLING PROPERTY… Unimproved properties, railroads and\n",
            "utilities (but not buildings) may be sold to any player as a private\n",
            "transaction for any amount the owner can get; however, no property\n",
            "can be sold to another player if buildings are standing on any\n",
            "properties of that color-group. Any buildings so located must be sold\n",
            "back to the Bank before the owner can sell any property of that color-group.\n",
            "Houses and hotels may be sold back to the Bank at any time for\n",
            "one-h...\n"
          ]
        }
      ],
      "source": [
        "faiss_memory = create_memory_vectorstore(\n",
        "    embedding_model=EMBEDDING_MODEL, path=\"memory_vectorstore\"\n",
        ")\n",
        "\n",
        "# Test some queries\n",
        "test_queries = [\n",
        "    \"what is EX card?\",\n",
        "    \"what is haste?\",\n",
        "    \"how to build a hotel?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n🔍 Query: {query}\")\n",
        "    memory_context = get_relevant_memory(query, faiss_memory)\n",
        "    result = rag_chain({\"question\": query, \"chat_history\": []})\n",
        "    print(f\"\\n🔍 Updated Query: \"+result[\"standalone_question\"])\n",
        "    print_result_summary(result)\n",
        "    store_to_memory_vectorstore(\n",
        "        question=query,\n",
        "        answer=result[\"answer\"],\n",
        "        vectorstore=faiss_memory,         \n",
        "        embedding_model=EMBEDDING_MODEL\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewritten prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Answer:\n",
            " (Alternative)\n",
            "\n",
            "An EX card is a type of Pokemon card in the Pokemon Trading Card Game that represents a more powerful version of a Pokemon. These cards were introduced in the EX Series and have distinct characteristics, such as a larger size, a special symbol (EX), and unique abilities. When an EX card is Knocked Out, the opponent takes two Prize cards instead of one. Examples of EX cards include Yveltal-EX and Charizard-EX.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 27\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "28\n",
            "Pokémon T rading Card Game Rules\n",
            "APPENDIX J: POKÉMON-EX\n",
            "Pokémon-EX are powerful Pokémon that show off a Pokémon with more HP and stronger attacks than regular \n",
            "Pokémon, but there are risks to playing these powered-up Pokémon!\n",
            "SPECIAL RULES FOR POKÉMON -EX\n",
            "The EX is part of a Pokémon-EX’s name. Thus Yveltal and Yveltal-EX \n",
            "have different names, so you can have up to 4 of each in your deck \n",
            "if you wish.\n",
            "When one of your Pokémon-EX is Knocked Out, your opponent \n",
            "takes 2 Prize cards.\n",
            "Apart from t...\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 32\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "POKÉMON:  The colorful characters that fight for you in the \n",
            "Pokémon Trading Card Game. They are represented in the game \n",
            "by Basic Pokémon and Evolution cards.\n",
            "POKÉMON-EX : Pokémon-EX are a stronger form of Pokémon \n",
            "with a special drawback: when your Pokémon-EX is Knocked Out, \n",
            "your opponent takes two Prize cards instead of one.\n",
            "POKÉMON-GX : Pokémon-GX are a stronger form of \n",
            "Pokémon with a special drawback: when your Pokémon-GX is \n",
            "Knocked Out, your opponent takes two Prize cards instead of one...\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 32\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "of Pokémon-EX with an additional drawback: when one  \n",
            "of your Pokémon becomes a Mega Evolution Pokémon, your  \n",
            "turn ends.\n",
            "OWNER: A Pokémon with a Trainer’s name in its title, such as \n",
            "Brock’s Sandshrew or Team Rocket’s Meowth. Cards with , , , \n",
            ", or  do NOT count as “Owner” cards.\n",
            "POISON MARKER:  Object put on a Pokémon to remind  \n",
            "you it is Poisoned. Remove the marker if the Pokémon is Benched \n",
            "or evolved.\n",
            "POKÉ-BODY: An effect that is active as soon as that Pokémon \n",
            "is in play and lasts until t...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Answer:\n",
            "\n",
            "\n",
            "Haste is a static ability in the Magic: The Gathering card game that allows a creature to attack even if it hasn't been controlled by its controller continuously since their most recent turn began. It doesn't have any direct impact on a character in a role-playing game, but it can be simulated by granting a character an extra action or allowing them to take actions out of turn. However, it's important to note that the rules for haste in Magic: The Gathering and in role-playing games may differ significantly.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 102\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "702.8b Multiple instances of flash on the same object are redundant. \n",
            " \n",
            "702.9. Flying \n",
            " \n",
            "702.9a Flying is an evasion ability. \n",
            " \n",
            "702.9b A creature with flying can’t be blocked except by creatures with flying and/or reach. A \n",
            "creature with flying can block a creature with or without flying. (See rule 509, “Declare \n",
            "Blockers Step,” and rule 702.17, “Reach.”) \n",
            " \n",
            "702.9c Multiple instances of flying on the same creature are redundant. \n",
            " \n",
            "702.10. Haste \n",
            " \n",
            "702.10a Haste is a static ability. \n",
            " \n",
            "702.10b ...\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 172\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "Defending Player \n",
            "The player who can be attacked, and whose planeswalkers can be attacked, during the combat phase. See \n",
            "rule 506.2. In certain multiplayer games, there may be more than one defending player; see rule 802, \n",
            "“Attack Multiple Players Option,” and rule 810.7. \n",
            " \n",
            "Defending Team \n",
            "The team who can be attacked, and whose planeswalkers can be attacked, during the combat phase of a \n",
            "Two-Headed Giant game. See rule 810.7. \n",
            " \n",
            "Delayed Triggered Ability \n",
            "An ability created by effects generate...\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 116\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "this card by putting it onto the stack from your hand, you may pay [cost] and exile it with N \n",
            "time counters on it. This action doesn’t use the stack,” and “At the beginning of your upkeep, if \n",
            "this card is suspended, remove a time counter from it,” and “When the last time counter is \n",
            "removed from this card, if it’s exiled, play it without paying its mana cost if able. If you can’t, it \n",
            "remains exiled. If you cast a creature spell this way, it gains haste until you lose control of the \n",
            "spell or ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Answer:\n",
            "\n",
            "\n",
            "To build a luxury hotel from the ground up in Monopoly, follow these steps:\n",
            "\n",
            "1. Acquire a complete color-group of properties.\n",
            "2. Build four houses on each property of the color-group.\n",
            "3. Once you have four houses on each property, you may buy a hotel from the Bank and erect it on any property of the color-group.\n",
            "4. Return the four houses from that property to the Bank and pay the price for the hotel as shown on the Title Deed card.\n",
            "5. Ensure there are houses and hotels available for sale in the Bank before proceeding.\n",
            "6. If necessary, participate in an auction to purchase the hotel if there are multiple players vying for the same property.\n",
            "7. Sell back any unnecessary houses or hotels to the Bank at any time for one-half the price paid for them.\n",
            "8. Remember that only one hotel may be erected on any one property.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 2\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "HOTELS...When a player has four houses on each property of a complete color-group, he may buy a hotel from the Bank and erect it on \n",
            "any property of that color-group. He returns the four houses from that property to the Bank and pays the price for the hotel as shown on \n",
            "the Title Deed card. Only one hotel may be erected on any one property.  \n",
            " \n",
            "BUILDING SHORTAGE...When the Bank has no houses to sell, players wishing to build must wait for some player to turn back or to \n",
            "sell his houses to the Ba...\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "houses back to the Bank (see SELLING PROPERTY).\n",
            "HOTELS… When a player has four houses on each property of a\n",
            "complete color-group, he/she may buy a hotel from the Bank and\n",
            "erect it on any property of the color-group. He/she returns the four\n",
            "houses from that property to the Bank and pays the price for the hotel\n",
            "as shown on the Title Deed card. Only one hotel may be erected on\n",
            "any one property.\n",
            "BUILDING SHORTAGES… When the Bank has no houses to sell,\n",
            "players wishing to build must wait for some play...\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: N/A\n",
            "📝 Excerpt:\n",
            "auction to the highest bidder.\n",
            "SELLING PROPERTY… Unimproved properties, railroads and\n",
            "utilities (but not buildings) may be sold to any player as a private\n",
            "transaction for any amount the owner can get; however, no property\n",
            "can be sold to another player if buildings are standing on any\n",
            "properties of that color-group. Any buildings so located must be sold\n",
            "back to the Bank before the owner can sell any property of that color-group.\n",
            "Houses and hotels may be sold back to the Bank at any time for\n",
            "one-h...\n"
          ]
        }
      ],
      "source": [
        "test_queries = [\n",
        "    \"what is EX card?\",\n",
        "    \"what is haste?\",\n",
        "    \"how to build a hotel?\"\n",
        "]\n",
        "for query in test_queries:\n",
        "    result = rag_chain({\"question\": query, \"chat_history\": []})\n",
        "    print_result_summary(result)\n",
        "    store_to_memory_vectorstore(\n",
        "        question=query,\n",
        "        answer=result[\"answer\"],\n",
        "        vectorstore=faiss_memory,         \n",
        "        embedding_model=EMBEDDING_MODEL\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAGAS EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Example evaluation data\n",
        "eval_data = [\n",
        "    {\n",
        "        \"question\": \"What is a Pokémon EX card?\",\n",
        "        \"answer\": \"A Pokémon-EX card is a powerful type of card with higher HP and stronger attacks. When it’s knocked out, the opponent takes 2 Prize cards.\",\n",
        "        \"contexts\": [\n",
        "            \"Pokémon-EX cards have more HP and stronger attacks than regular Pokémon cards. If one is knocked out, the opponent takes 2 Prize cards.\"\n",
        "        ],\n",
        "        \"reference\": \"Pokémon-EX cards are a special kind of card that have more HP and stronger attacks than normal cards. When knocked out, they give the opponent 2 Prize cards.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What does haste do in Magic: The Gathering?\",\n",
        "        \"answer\": \"Haste allows creatures to attack and use abilities immediately after being played, ignoring summoning sickness.\",\n",
        "        \"contexts\": [\n",
        "            \"Creatures with haste can attack or use tap/untap abilities even if they haven't been under the player's control since the beginning of the turn.\"\n",
        "        ],\n",
        "        \"reference\": \"Haste is a keyword that allows a creature to attack or use abilities as soon as it enters the battlefield, bypassing summoning sickness.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can a player build a hotel in Monopoly?\",\n",
        "        \"answer\": \"To build a hotel, a player must first own all properties in a color group and build four houses on each property. Then, they can exchange the houses for a hotel.\",\n",
        "        \"contexts\": [\n",
        "            \"Once a player has four houses on each property in a color group, they may buy a hotel and place it on any of those properties. The four houses are returned to the bank.\"\n",
        "        ],\n",
        "        \"reference\": \"A player must have four houses on every property in a color set to buy a hotel. The player exchanges the houses for one hotel and pays the bank the cost listed on the title deed.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_dataset = Dataset.from_list(eval_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
        "\n",
        "metrics = [faithfulness, answer_relevancy, context_precision]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3abe574631a4fad94b5e4cc01568dae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[6]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[4]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[1]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[7]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[8]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[2]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "Exception raised in Job[0]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[3]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n",
            "c:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._generated._async_client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "Exception raised in Job[5]: ClientResponseError(402, message='Payment Required', url='https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'faithfulness': nan, 'answer_relevancy': nan, 'context_precision': nan}\n"
          ]
        }
      ],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "# Wrap the LLM\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "# Evaluate the dataset\n",
        "results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm)\n",
        "\n",
        "# Display the results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import product\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from sentence_transformers import CrossEncoder\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
        "from datasets import Dataset\n",
        "\n",
        "# Assuming existing helper functions are imported:\n",
        "# get_prompt(), create_parent_document_reranker(), create_memory_vectorstore(), get_relevant_memory(), store_to_memory_vectorstore()\n",
        "# and previously defined create_rag_chain().\n",
        "\n",
        "queries = [\n",
        "    \"what is EX card?\",\n",
        "    \"what is haste?\",\n",
        "    \"how to build a hotel?\"\n",
        "]\n",
        "\n",
        "prompt_types = [\"zero_shot\", \"react\", \"cot\"]\n",
        "\n",
        "configurations = [\n",
        "    {\"use_memory\": False, \"rewrite_prompt\": False, \"use_reranking\": False, \"prompt_type\": p} for p in prompt_types]\n",
        "#  + [\n",
        "#     {\"use_memory\": False, \"rewrite_prompt\": True, \"use_reranking\": False, \"prompt_type\": p} for p in prompt_types\n",
        "# ] + [\n",
        "#     {\"use_memory\": True, \"rewrite_prompt\": False, \"use_reranking\": False, \"prompt_type\": p} for p in prompt_types\n",
        "# ] + [\n",
        "#     {\"use_memory\": True, \"rewrite_prompt\": True, \"use_reranking\": False, \"prompt_type\": p} for p in prompt_types\n",
        "# ] + [\n",
        "#     {\"use_memory\": False, \"rewrite_prompt\": False, \"use_reranking\": True, \"prompt_type\": p} for p in prompt_types\n",
        "# ] + [\n",
        "#     {\"use_memory\": True, \"rewrite_prompt\": True, \"use_reranking\": True, \"prompt_type\": p} for p in prompt_types\n",
        "# ]\n",
        "\n",
        "EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "faiss_memory = create_memory_vectorstore(embedding_model=EMBEDDING_MODEL, path=\"memory_vectorstore\")\n",
        "\n",
        "wrapped_llm = LangchainLLMWrapper(llm)\n",
        "metrics = [faithfulness, answer_relevancy]\n",
        "\n",
        "eval_data = [\n",
        "    {\n",
        "        \"question\": \"What is a Pokémon EX card?\",\n",
        "        \"answer\": \"A Pokémon-EX card is a powerful type of card with higher HP and stronger attacks. When it’s knocked out, the opponent takes 2 Prize cards.\",\n",
        "        \"contexts\": [\n",
        "            \"Pokémon-EX cards have more HP and stronger attacks than regular Pokémon cards. If one is knocked out, the opponent takes 2 Prize cards.\"\n",
        "        ],\n",
        "        \"reference\": \"Pokémon-EX cards are a special kind of card that have more HP and stronger attacks than normal cards. When knocked out, they give the opponent 2 Prize cards.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What does haste do in Magic: The Gathering?\",\n",
        "        \"answer\": \"Haste allows creatures to attack and use abilities immediately after being played, ignoring summoning sickness.\",\n",
        "        \"contexts\": [\n",
        "            \"Creatures with haste can attack or use tap/untap abilities even if they haven't been under the player's control since the beginning of the turn.\"\n",
        "        ],\n",
        "        \"reference\": \"Haste is a keyword that allows a creature to attack or use abilities as soon as it enters the battlefield, bypassing summoning sickness.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can a player build a hotel in Monopoly?\",\n",
        "        \"answer\": \"To build a hotel, a player must first own all properties in a color group and build four houses on each property. Then, they can exchange the houses for a hotel.\",\n",
        "        \"contexts\": [\n",
        "            \"Once a player has four houses on each property in a color group, they may buy a hotel and place it on any of those properties. The four houses are returned to the bank.\"\n",
        "        ],\n",
        "        \"reference\": \"A player must have four houses on every property in a color set to buy a hotel. The player exchanges the houses for one hotel and pays the bank the cost listed on the title deed.\"\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "HfHubHTTPError",
          "evalue": "402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Root=1-67fc22f2-70de8ff36cf19ed05e496a48;8dd2ed09-f643-4d90-8d64-f0b3c0de3172)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[1;31mHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 54\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configurations:\n\u001b[0;32m     49\u001b[0m         rag_chain, memory \u001b[38;5;241m=\u001b[39m create_rag_chain(\n\u001b[0;32m     50\u001b[0m             vectorstore\u001b[38;5;241m=\u001b[39mfaiss_memory,\n\u001b[0;32m     51\u001b[0m             llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m     52\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\n\u001b[0;32m     53\u001b[0m         )\n\u001b[1;32m---> 54\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m         eval_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     57\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m     58\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiscriminator_eval\u001b[39m\u001b[38;5;124m\"\u001b[39m: hf_discriminator_eval(query, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     62\u001b[0m         })\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Convert eval_data to the format expected by RAGAS\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\conversational_retrieval\\base.py:170\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    168\u001b[0m         new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_question\n\u001b[0;32m    169\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m chat_history_str\n\u001b[1;32m--> 170\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs_chain\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    171\u001b[0m         input_documents\u001b[38;5;241m=\u001b[39mdocs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_inputs\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    173\u001b[0m     output[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m answer\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:611\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[0;32m    607\u001b[0m         _output_key\n\u001b[0;32m    608\u001b[0m     ]\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[0;32m    612\u001b[0m         _output_key\n\u001b[0;32m    613\u001b[0m     ]\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    617\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    618\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    619\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[0;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[1;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcombine_docs(\n\u001b[0;32m    139\u001b[0m     docs, callbacks\u001b[38;5;241m=\u001b[39m_run_manager\u001b[38;5;241m.\u001b[39mget_child(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mother_keys\n\u001b[0;32m    140\u001b[0m )\n\u001b[0;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\combine_documents\\stuff.py:259\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[1;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39mpredict(callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs), {}\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:318\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    304\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:181\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     emit_warning()\n\u001b[1;32m--> 181\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    139\u001b[0m         prompts,\n\u001b[0;32m    140\u001b[0m         stop,\n\u001b[0;32m    141\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    143\u001b[0m     )\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:763\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    757\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    761\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    762\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:966\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    953\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    954\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    964\u001b[0m         )\n\u001b[0;32m    965\u001b[0m     ]\n\u001b[1;32m--> 966\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    967\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    968\u001b[0m     )\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    779\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    786\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 787\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    788\u001b[0m                 prompts,\n\u001b[0;32m    789\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    790\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    791\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    792\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    793\u001b[0m             )\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    796\u001b[0m         )\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    798\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1526\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1523\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1525\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1526\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1527\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1528\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m     )\n\u001b[0;32m   1530\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:312\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    311\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:132\u001b[0m, in \u001b[0;36m_deprecate_method.<locals>._inner_deprecate_method.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    130\u001b[0m     warning_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m message\n\u001b[0;32m    131\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(warning_message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:305\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    303\u001b[0m url \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39m_prepare_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken, mapped_model)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    304\u001b[0m headers \u001b[38;5;241m=\u001b[39m provider_helper\u001b[38;5;241m.\u001b[39m_prepare_headers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRequestParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munknown\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:357\u001b[0m, in \u001b[0;36mInferenceClient._inner_post\u001b[1;34m(self, request_parameters, stream)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequest_parameters\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 357\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
            "File \u001b[1;32mc:\\Users\\oskar\\PycharmProjects\\pythonProject\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:482\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[1;31mHfHubHTTPError\u001b[0m: 402 Client Error: Payment Required for url: https://router.huggingface.co/hf-inference/models/mistralai/Mistral-7B-Instruct-v0.2 (Request ID: Root=1-67fc22f2-70de8ff36cf19ed05e496a48;8dd2ed09-f643-4d90-8d64-f0b3c0de3172)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from huggingface_hub import InferenceClient\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Initialize Hugging Face client for discriminator\n",
        "hf_api_key = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
        "hf_model_name = \"mistralai/Mistral-7B-Instruct-v0.2\" # Replace with your preferred model\n",
        "discriminator_client = InferenceClient(\n",
        "    model=hf_model_name,\n",
        "    token=hf_api_key\n",
        ")\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    task=\"text-generation\",\n",
        "    huggingfacehub_api_token=hf_api_key,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512\n",
        ")\n",
        "# Function to use HuggingFace as discriminator\n",
        "def hf_discriminator_eval(query, answer):\n",
        "    try:\n",
        "        prompt = f\"\"\"\n",
        "        Question: {query} \n",
        "        Answer: {answer} \n",
        "        \n",
        "        Is this answer accurate and helpful? Respond with 1 to 5, where 1 is not helpful at all and 5 is very helpful.\n",
        "        \"\"\"\n",
        "        \n",
        "        response = discriminator_client.text_generation(\n",
        "            prompt,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.1,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "        \n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(f\"Error with discriminator evaluation: {e}\")\n",
        "        return \"Error in evaluation\"\n",
        "\n",
        "# Main evaluation loop\n",
        "for query in queries:\n",
        "    for config in configurations:\n",
        "        rag_chain, memory = create_rag_chain(\n",
        "            vectorstore=faiss_memory,\n",
        "            llm=llm,\n",
        "            **config\n",
        "        )\n",
        "        result = rag_chain({\"question\": query, \"chat_history\": []})\n",
        "\n",
        "        eval_data.append({\n",
        "            \"question\": query,\n",
        "            \"answer\": result[\"answer\"],\n",
        "            \"contexts\": [doc.page_content for doc in result[\"source_documents\"]],\n",
        "            \"configuration\": str(config),\n",
        "            \"discriminator_eval\": hf_discriminator_eval(query, result[\"answer\"])\n",
        "        })\n",
        "\n",
        "# Convert eval_data to the format expected by RAGAS\n",
        "eval_dataset = Dataset.from_list([\n",
        "    {\n",
        "        \"question\": row[\"question\"],\n",
        "        \"answer\": row[\"answer\"],\n",
        "        \"contexts\": row[\"contexts\"],\n",
        "        \"configuration\": row.get(\"configuration\", \"manual_eval\"),\n",
        "        \"discriminator_eval\": row.get(\"discriminator_eval\", \"\")\n",
        "    } for row in eval_data\n",
        "])\n",
        "\n",
        "\n",
        "# Run RAGAS evaluation\n",
        "ragas_scores = evaluate(dataset=eval_dataset, metrics=metrics, llm=wrapped_llm)\n",
        "\n",
        "# Combine RAGAS results with metadata\n",
        "results = []\n",
        "for i, row in enumerate(eval_data):\n",
        "    results.append({\n",
        "        \"query\": row[\"question\"],\n",
        "        \"configuration\": row.get(\"configuration\", \"N/A\"),\n",
        "        \"answer\": row[\"answer\"],\n",
        "        \"faithfulness\": ragas_scores[\"faithfulness\"][i],\n",
        "        \"relevance\": ragas_scores[\"answer_relevancy\"][i],\n",
        "        \"context_precision\": ragas_scores[\"context_precision\"][i],\n",
        "        \"discriminator_eval\": row[\"discriminator_eval\"]\n",
        "    })\n",
        "\n",
        "# Generate DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display DataFrame\n",
        "display(results_df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
