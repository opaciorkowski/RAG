{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71f0917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm.model_name: gpt-3.5-turbo\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from advanced_rag import *\n",
    "import json\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745b23aa",
   "metadata": {},
   "source": [
    "## Creating necessery variables such as vectore base and declaring a llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c780a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new documents to add.\n"
     ]
    }
   ],
   "source": [
    "db, llm = initialize_rag(model_choice=\"gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674c29a4",
   "metadata": {},
   "source": [
    "## Create Evaluation Dataset\n",
    "\n",
    "We are building a dataset for RAGAS evaluation using questions and ground truth answers from `qa_eval.json`.\n",
    "\n",
    "For each entry, we will include:\n",
    "- **user_input**: the question\n",
    "- **response**: the answer generated by the LLM using retrieved context\n",
    "- **retrieved_contexts**: the documents returned by the retriever\n",
    "- **reference**: the ground truth answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "382ab76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_eval_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        eval_data = json.load(file)\n",
    "        queries = [entry[\"question\"] for entry in eval_data]\n",
    "        ground_truth = [entry[\"ground_truth\"] for entry in eval_data]\n",
    "    return queries, ground_truth\n",
    "\n",
    "def create_evaluation_dataset(answers, contexts, queries, ground_truth):\n",
    "    eval_dataset = []\n",
    "    for i in range(len(queries)):\n",
    "        eval_dataset.append({\n",
    "            \"user_input\": queries[i],\n",
    "            \"response\": answers[i],\n",
    "            \"retrieved_contexts\": contexts[i],\n",
    "            \"reference\": ground_truth[i]\n",
    "        })\n",
    "    return eval_dataset\n",
    "\n",
    "def extract_and_print_contexts(result):\n",
    "    contexts = [doc.page_content for doc in result.get(\"source_documents\", [])]\n",
    "    return contexts\n",
    "\n",
    "def run_queries(rag_chain, file_path):\n",
    "    results = []\n",
    "    contexts = []\n",
    "    queries, ground_truth = load_eval_data(file_path)\n",
    "    for query in queries:\n",
    "        result = rag_chain({\"question\": query})\n",
    "        results.append(result['answer'])\n",
    "        contexts.append(extract_and_print_contexts(result))\n",
    "    return create_evaluation_dataset(results, contexts, queries, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d1afa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(evaluation_dataset, llm):\n",
    "    evaluator_llm = LangchainLLMWrapper(llm)\n",
    "    print(type(evaluator_llm))\n",
    "    print(evaluator_llm)\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=[\n",
    "            LLMContextRecall(), \n",
    "            Faithfulness(), \n",
    "            FactualCorrectness()\n",
    "        ],\n",
    "        llm=evaluator_llm,\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0375f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset_to_df(dataset):\n",
    "    return pd.DataFrame([\n",
    "        {\n",
    "            \"Question\": entry[\"user_input\"],\n",
    "            \"Response\": entry[\"response\"],\n",
    "            \"Context (1st)\": entry[\"retrieved_contexts\"][0] if entry[\"retrieved_contexts\"] else \"\",\n",
    "            \"Ground Truth\": entry[\"reference\"],\n",
    "        }\n",
    "        for entry in dataset\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b78cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_result_to_df(results) -> pd.DataFrame:\n",
    "    scores_dict = results._scores_dict\n",
    "    df = pd.DataFrame(scores_dict).T\n",
    "    df.columns = [f\"q{i+1}\" for i in range(df.shape[1])]\n",
    "    df[\"avg\"] = df.mean(axis=1).round(4)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69c38fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:22: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n"
     ]
    }
   ],
   "source": [
    "rag_chain, rag_memory = create_rag_chain(\n",
    "    vectorstore=db,\n",
    "    llm=llm,\n",
    "    prompt_type='zero_shot',\n",
    "    use_memory=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86904b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oskar\\AppData\\Local\\Temp\\ipykernel_14636\\1773067024.py:28: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain({\"question\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ragas.llms.base.LangchainLLMWrapper'>\n",
      "LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5445b7df5fcb4a1ca0ec0ccffee01dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_recall': 1.0000, 'faithfulness': 0.7778, 'factual_correctness(mode=f1)': 0.6700}\n"
     ]
    }
   ],
   "source": [
    "file_path = 'qa_eval.json'\n",
    "dataset = run_queries(rag_chain, file_path)\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "results = evaluate_dataset(eval_dataset, llm)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01132154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.7778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.6700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                q1        q2    q3     avg\n",
       "context_recall                1.00  1.000000  1.00  1.0000\n",
       "faithfulness                  1.00  0.333333  1.00  0.7778\n",
       "factual_correctness(mode=f1)  0.67  0.670000  0.67  0.6700"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = evaluation_result_to_df(results)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54dbd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain, rag_memory = create_rag_chain(\n",
    "    vectorstore=db,\n",
    "    llm=llm,\n",
    "    prompt_type='cot',\n",
    "    use_memory=True,\n",
    "    use_reranking=True,\n",
    "    additional_prompt_instruction=\"provide only final answer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "390458e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ragas.llms.base.LangchainLLMWrapper'>\n",
      "LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34773005dbad46d78550d96125e4f011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_recall': 1.0000, 'faithfulness': 0.6111, 'factual_correctness(mode=f1)': 0.5567}\n"
     ]
    }
   ],
   "source": [
    "dataset = run_queries(rag_chain, file_path)\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "results = evaluate_dataset(eval_dataset, llm)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4c3544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    q1    q2   q3     avg\n",
       "context_recall                1.000000  1.00  1.0  1.0000\n",
       "faithfulness                  0.333333  1.00  0.5  0.6111\n",
       "factual_correctness(mode=f1)  1.000000  0.67  0.0  0.5567"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = evaluation_result_to_df(results)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "948e3ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain, rag_memory = create_rag_chain(\n",
    "    vectorstore=db,\n",
    "    llm=llm,\n",
    "    prompt_type='react',\n",
    "    use_memory=True,\n",
    "    use_reranking=True,\n",
    "    rewrite=True,\n",
    "    additional_prompt_instruction=\"provide only final answer\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2781dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten query: content='Can you provide detailed information on the characteristics and gameplay significance of Pokémon EX cards in the Pokémon Trading Card Game?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 85, 'total_tokens': 108, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BO98QtlAjqhEkbKFNDcvK3xoJcX9c', 'finish_reason': 'stop', 'logprobs': None} id='run-70b4925c-55a4-41a1-83e9-30404faea099-0' usage_metadata={'input_tokens': 85, 'output_tokens': 23, 'total_tokens': 108, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Rewritten content: Can you provide detailed information on the characteristics and gameplay significance of Pokémon EX cards in the Pokémon Trading Card Game?\n",
      "Rewritten query: content='How does the mechanic \"haste\" impact gameplay in Magic: The Gathering?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 88, 'total_tokens': 105, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BO98UCX7gcbPeS7ypcdf9eoleaN26', 'finish_reason': 'stop', 'logprobs': None} id='run-7e353dc1-30b4-41a9-ab52-21d9d7a8fbf3-0' usage_metadata={'input_tokens': 88, 'output_tokens': 17, 'total_tokens': 105, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Rewritten content: How does the mechanic \"haste\" impact gameplay in Magic: The Gathering?\n",
      "Rewritten query: content='What are the specific steps and strategies for a player to successfully build a hotel in the game of Monopoly?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 89, 'total_tokens': 112, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BO98YQo6qQs1LAWzCIwhxM6W1VQxS', 'finish_reason': 'stop', 'logprobs': None} id='run-6d7ae958-cac4-4463-b2c2-915e5d65996b-0' usage_metadata={'input_tokens': 89, 'output_tokens': 23, 'total_tokens': 112, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Rewritten content: What are the specific steps and strategies for a player to successfully build a hotel in the game of Monopoly?\n",
      "<class 'ragas.llms.base.LangchainLLMWrapper'>\n",
      "LangchainLLMWrapper(langchain_llm=ChatOpenAI(...))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365750b467be429b867099ea2ea479c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_recall': 1.0000, 'faithfulness': 0.7000, 'factual_correctness(mode=f1)': 0.8700}\n"
     ]
    }
   ],
   "source": [
    "dataset = run_queries(rag_chain, file_path)\n",
    "eval_dataset = EvaluationDataset.from_list(dataset)\n",
    "results = evaluate_dataset(eval_dataset, llm)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32a917cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               q1    q2    q3   avg\n",
       "context_recall                1.0  1.00  1.00  1.00\n",
       "faithfulness                  0.6  0.50  1.00  0.70\n",
       "factual_correctness(mode=f1)  1.0  0.75  0.86  0.87"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3 = evaluation_result_to_df(results)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428cff05",
   "metadata": {},
   "source": [
    "## SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6718624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Simple RAG + Zero-Shot</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Reranking + CoT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">Reranking, Query Rewriting + React</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>avg</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>avg</th>\n",
       "      <th>q1</th>\n",
       "      <th>q2</th>\n",
       "      <th>q3</th>\n",
       "      <th>avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.7778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6111</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.6700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Simple RAG + Zero-Shot                          \\\n",
       "                                                 q1        q2    q3     avg   \n",
       "context_recall                                 1.00  1.000000  1.00  1.0000   \n",
       "faithfulness                                   1.00  0.333333  1.00  0.7778   \n",
       "factual_correctness(mode=f1)                   0.67  0.670000  0.67  0.6700   \n",
       "\n",
       "                             Reranking + CoT                     \\\n",
       "                                          q1    q2   q3     avg   \n",
       "context_recall                      1.000000  1.00  1.0  1.0000   \n",
       "faithfulness                        0.333333  1.00  0.5  0.6111   \n",
       "factual_correctness(mode=f1)        1.000000  0.67  0.0  0.5567   \n",
       "\n",
       "                             Reranking, Query Rewriting + React              \\\n",
       "                                                             q1    q2    q3   \n",
       "context_recall                                              1.0  1.00  1.00   \n",
       "faithfulness                                                0.6  0.50  1.00   \n",
       "factual_correctness(mode=f1)                                1.0  0.75  0.86   \n",
       "\n",
       "                                    \n",
       "                               avg  \n",
       "context_recall                1.00  \n",
       "faithfulness                  0.70  \n",
       "factual_correctness(mode=f1)  0.87  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the dataframes\n",
    "merged_df = pd.concat([df1, df2, df3], axis=1)\n",
    "\n",
    "# Create MultiIndex with labels for each group\n",
    "n_cols = df1.shape[1]  # number of columns per df\n",
    "\n",
    "merged_df.columns = pd.MultiIndex.from_tuples(\n",
    "    [(\"Simple RAG + Zero-Shot\", col) for col in df1.columns] +\n",
    "    [(\"Reranking + CoT\", col) for col in df2.columns] +\n",
    "    [(\"Reranking, Query Rewriting + React\", col) for col in df3.columns]\n",
    ")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ef354fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reranking + CoT</th>\n",
       "      <th>Reranking, Query Rewriting + React</th>\n",
       "      <th>Simple RAG + Zero-Shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>context_recall</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>faithfulness</th>\n",
       "      <td>0.611108</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.777783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factual_correctness(mode=f1)</th>\n",
       "      <td>0.556675</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.670000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Reranking + CoT  \\\n",
       "context_recall                       1.000000   \n",
       "faithfulness                         0.611108   \n",
       "factual_correctness(mode=f1)         0.556675   \n",
       "\n",
       "                              Reranking, Query Rewriting + React  \\\n",
       "context_recall                                              1.00   \n",
       "faithfulness                                                0.70   \n",
       "factual_correctness(mode=f1)                                0.87   \n",
       "\n",
       "                              Simple RAG + Zero-Shot  \n",
       "context_recall                              1.000000  \n",
       "faithfulness                                0.777783  \n",
       "factual_correctness(mode=f1)                0.670000  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = merged_df.T.groupby(level=0).mean().T\n",
    "\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
