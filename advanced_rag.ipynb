{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System with Gemini and FAISS\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Union\n",
        "\n",
        "import requests\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document, BaseRetriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "First, let's set up the configuration for our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Configuration settings\n",
        "EMBEDDING = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name=EMBEDDING)\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 80\n",
        "RETRIEVER_K = 4\n",
        "# GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "# if not GEMINI_API_KEY:\n",
        "#     raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
        "\n",
        "# llm = GoogleGenerativeAI(\n",
        "#     api_key=GEMINI_API_KEY,\n",
        "#     model=\"gemini-2.0-flash\",\n",
        "#     verbose=False\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llm.model_name: gpt-3.5-turbo\n"
          ]
        }
      ],
      "source": [
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY:\n",
        "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    api_key=OPENAI_API_KEY,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.0,\n",
        "    verbose=False\n",
        ")\n",
        "try:\n",
        "    print(\"llm.model_name:\", llm.model_name)\n",
        "except AttributeError:\n",
        "    print(\"llm.model:\", llm.model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document Processing Functions\n",
        "Let's define functions to download and process PDF documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_pdf(url: str, folder: str = 'documents') -> str:\n",
        "    \"\"\"\n",
        "    Downloads PDF from given URL\n",
        "    \"\"\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    filename = os.path.basename(url.split('?')[0])\n",
        "    filepath = os.path.join(folder, filename)\n",
        "\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    return filepath\n",
        "\n",
        "def process_document(\n",
        "    documents: Union[List[str], List[Document]], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process a list of Document objects or URLs into chunks while preserving parent document relationships.\n",
        "    \"\"\"\n",
        "    if isinstance(chunk_size, str):\n",
        "        chunk_size = int(chunk_size)\n",
        "    if isinstance(chunk_overlap, str):\n",
        "        chunk_overlap = int(chunk_overlap)\n",
        "\n",
        "    if documents and isinstance(documents[0], str):\n",
        "        loaded_docs = []\n",
        "        for url in documents:\n",
        "            pdf_path = download_pdf(url)\n",
        "            pdf_docs = PyPDFLoader(pdf_path).load()\n",
        "            loaded_docs.extend(pdf_docs)\n",
        "        documents = loaded_docs\n",
        "\n",
        "    def get_filename(path):\n",
        "        if not path or path == \"unknown\":\n",
        "            return \"unknown_document\"\n",
        "        return os.path.basename(path).split('.')[0]\n",
        "\n",
        "    source_groups = {}\n",
        "    for doc in documents:\n",
        "        source = doc.metadata.get(\"source\", \"unknown\")\n",
        "        source_groups.setdefault(source, []).append(doc)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "    for source, docs in source_groups.items():\n",
        "        docs.sort(key=lambda x: x.metadata.get(\"page\", 0))\n",
        "        parent_id = get_filename(source)\n",
        "        chunks = splitter.split_documents(docs)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            page_num = chunk.metadata.get(\"page\", 0)\n",
        "            chunk_id = f\"{parent_id}_p{page_num}_c{i}\"\n",
        "            chunk.metadata.update({\n",
        "                \"parent_id\": parent_id,\n",
        "                \"parent_source\": source,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": len(chunks)\n",
        "            })\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vector Store Creation\n",
        "Now let's create a function to build our vector store from documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_and_store_documents(documents, chunk_size, chunk_overlap,\n",
        "                                embedding_model=EMBEDDING_MODEL,\n",
        "                                persist_directory=\"faiss_index\") -> FAISS:\n",
        "    \"\"\"\n",
        "    Process documents into chunks and add to FAISS vector DB,\n",
        "    skipping documents with existing parent_id.\n",
        "    \"\"\"\n",
        "    from langchain.vectorstores.faiss import FAISS\n",
        "    import os\n",
        "\n",
        "    chunks = process_document(documents, chunk_size, chunk_overlap)\n",
        "\n",
        "    if os.path.exists(persist_directory):\n",
        "        vector_store = FAISS.load_local(persist_directory, embedding_model, allow_dangerous_deserialization=True)\n",
        "        existing_ids = {\n",
        "            doc.metadata[\"parent_id\"]\n",
        "            for doc in vector_store.docstore._dict.values()\n",
        "            if \"parent_id\" in doc.metadata\n",
        "        }\n",
        "    else:\n",
        "        vector_store = None\n",
        "        existing_ids = set()\n",
        "\n",
        "    new_chunks = [c for c in chunks if c.metadata[\"parent_id\"] not in existing_ids]\n",
        "\n",
        "    if not new_chunks:\n",
        "        print(\"No new documents to add.\")\n",
        "        return vector_store\n",
        "\n",
        "    if vector_store:\n",
        "        vector_store.add_documents(new_chunks)\n",
        "    else:\n",
        "        vector_store = FAISS.from_documents(new_chunks, embedding_model)\n",
        "\n",
        "    vector_store.save_local(persist_directory)\n",
        "    return vector_store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Long-Term memory storage in Vectorstore\n",
        "This part is created to store conversation with LLM as additional context to be provided during retriving data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_memory_vectorstore(embedding_model, path=\"memory_vectorstore\"):\n",
        "    if Path(path).exists():\n",
        "        return FAISS.load_local(path, embedding_model, allow_dangerous_deserialization=True)\n",
        "    \n",
        "    dim = len(embedding_model.embed_query(\"test\"))\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "    docstore = InMemoryDocstore()\n",
        "    index_to_docstore_id = {}\n",
        "    \n",
        "    return FAISS(\n",
        "        embedding_function=embedding_model,\n",
        "        index=index,\n",
        "        docstore=docstore,\n",
        "        index_to_docstore_id=index_to_docstore_id\n",
        "    )\n",
        "\n",
        "def store_to_memory_vectorstore(question, answer, vectorstore):\n",
        "    content = f\"Q: {question}\\nA: {answer}\"\n",
        "    doc = Document(page_content=content, metadata={\"type\": \"chat_memory\"})\n",
        "    vectorstore.add_documents([doc])\n",
        "    vectorstore.save_local(\"memory_vectorstore\")\n",
        "    \n",
        "def get_relevant_memory(query, vectorstore, k=3):\n",
        "    try:\n",
        "        memory_docs = vectorstore.similarity_search(query, k=k)\n",
        "        return \"\\n---\\n\".join([doc.page_content for doc in memory_docs])\n",
        "    except (IndexError, ValueError) as e:\n",
        "        print(f\"Skipping memory retrieval (reason: {str(e)})\")\n",
        "        return \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_parent_document_llm_reranker(vectorstore, top_k_chunks=20, top_k_parents=4):\n",
        "    class LLMRerankerRetriever(BaseRetriever):\n",
        "        def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "            relevant_chunks_with_scores = vectorstore.similarity_search_with_score(query, k=top_k_chunks)\n",
        "            chunks = [doc for doc, _ in relevant_chunks_with_scores]\n",
        "            scores = [score for _, score in relevant_chunks_with_scores]\n",
        "\n",
        "            parent_docs = {}\n",
        "            for chunk, score in zip(chunks, scores):\n",
        "                parent_id = chunk.metadata.get(\"parent_id\") or chunk.metadata.get(\"doc_id\", f\"doc_{len(parent_docs)}\")\n",
        "                if parent_id not in parent_docs:\n",
        "                    parent_docs[parent_id] = {\n",
        "                        \"chunks\": [],\n",
        "                        \"scores\": [],\n",
        "                        \"source\": chunk.metadata.get(\"parent_source\", \"unknown\")\n",
        "                    }\n",
        "                parent_docs[parent_id][\"chunks\"].append(chunk)\n",
        "                parent_docs[parent_id][\"scores\"].append(score)\n",
        "\n",
        "            parent_list = []\n",
        "            for parent_id, parent in parent_docs.items():\n",
        "                parent[\"chunks\"].sort(key=lambda x: (x.metadata.get(\"page\", 0), x.metadata.get(\"chunk_index\", 0)))\n",
        "                full_text = \"\\n\".join([chunk.page_content for chunk in parent[\"chunks\"]])\n",
        "\n",
        "                try:\n",
        "                    rerank_score = float(cross_encoder.predict([(query, full_text)])[0])\n",
        "                except Exception:\n",
        "                    rerank_score = 0.0\n",
        "\n",
        "                for c in parent[\"chunks\"]:\n",
        "                    c.metadata[\"rerank_score\"] = rerank_score\n",
        "\n",
        "                parent_list.append({\n",
        "                    \"id\": parent_id,\n",
        "                    \"chunks\": parent[\"chunks\"],\n",
        "                    \"rerank_score\": rerank_score,\n",
        "                    \"source\": parent[\"source\"]\n",
        "                })\n",
        "\n",
        "            parent_list.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "            top_docs = []\n",
        "            for parent in parent_list[:top_k_parents]:\n",
        "                top_docs.extend(parent[\"chunks\"])\n",
        "\n",
        "            return top_docs\n",
        "\n",
        "        async def _aget_relevant_documents(self, query: str):\n",
        "            raise NotImplementedError(\"Async version not implemented.\")\n",
        "\n",
        "    return LLMRerankerRetriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Different types of prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_prompt(prompt_type: str, role: str = \"Ai Assistant\") -> PromptTemplate:\n",
        "    if prompt_type == \"zero_shot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Be clear and concise.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"explain_like_5\":\n",
        "        template = \"\"\"Use the following pieces of context to answer the question. Explain like you are talking to a 5-year-old. \n",
        "        If the question is not related to the context, say \"I don't know\". If you don't know the answer, just say that you don't know.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Provide a clear and concise answer.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"cot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Think step-by-step and explain your reasoning.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Let's think step by step:\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"elaborate\":\n",
        "        template = \"\"\"Use the following context to answer the question in a detailed, formal tone. If you can't answer, say \"I don't know\".\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Detailed answer:\"\"\"\n",
        "    elif prompt_type == \"meta\":\n",
        "        template = \"\"\"You are an AI assistant tasked with answering the question using the provided context. \n",
        "        First, generate an optimal prompt that would help an LLM perform this task effectively.\n",
        "        Then, respond to that prompt yourself to complete the task.\n",
        "        Reflect on your reasoning process as you answer. Clearly state what you know, what you are assuming, and how confident you are.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer (include reasoning, assumptions, and confidence level):\"\"\"\n",
        "    elif prompt_type == \"role\":\n",
        "        template = \"\"\"You are acting as {role}. Use the following context to answer the question appropriately for your role.\n",
        "        If the question is not related to the context, say \"I don't know\". If you're unsure of the answer, acknowledge that.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        As a {role}, your answer:\"\"\"\n",
        "    elif prompt_type == \"react\":\n",
        "        template = \"\"\"You are an intelligent assistant that reasons step-by-step and can use external context to answer questions.\n",
        "\n",
        "        Use the following format:\n",
        "\n",
        "        Question: {question}\n",
        "        Thought: Think about what you need to find.\n",
        "        Action: Look up relevant information in the context.\n",
        "        Observation: Summarize what the context says.\n",
        "        Thought: Reflect on how the observation answers the question.\n",
        "        Final Answer: Give a complete and clear answer.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Now follow the steps to answer:\n",
        "        \"\"\"\n",
        "    elif prompt_type == \"verify\":\n",
        "        template = \"\"\"\n",
        "        Given the context and the answer, verify if the answer is fully supported. \n",
        "        Respond with YES or NO, then explain briefly.\n",
        "        \n",
        "        Context:\n",
        "        {context}\n",
        "        \n",
        "        Answer:\n",
        "        {answer}\n",
        "        \n",
        "        Is this answer verifiable?\n",
        "        \"\"\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n",
        "\n",
        "    return PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG Chain Creation\n",
        "Let's create our RAG chain with the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_chain(\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt_type: str = \"zero_shot\",\n",
        "    use_memory: bool = False,\n",
        "    use_reranking: bool = False,\n",
        "    retriever_k: int = 4,\n",
        "    top_k_chunks: int = 20\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a RAG chain with optional memory, reranking, and query rewriting.\n",
        "    \"\"\"\n",
        "    prompt = get_prompt(prompt_type)\n",
        "\n",
        "    memory = (\n",
        "        ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            input_key=\"question\",\n",
        "            output_key=\"answer\",\n",
        "            return_messages=True\n",
        "        )\n",
        "        if use_memory else None\n",
        "    )\n",
        "\n",
        "    if use_reranking:\n",
        "        retriever = create_parent_document_llm_reranker(\n",
        "            vectorstore=vectorstore,\n",
        "            top_k_chunks=top_k_chunks,\n",
        "            top_k_parents=retriever_k\n",
        "        )\n",
        "    else:\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": retriever_k}\n",
        "        )\n",
        "\n",
        "    base_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return base_chain, memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_result_summary(result):\n",
        "    print(f\"\\n🧠 Answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "    print(\"🔎 Retrieved Documents:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "        print(f\"\\n📄 Chunk #{i}\")\n",
        "        print(f\"📚 Source: {doc.metadata.get('parent_source', 'unknown')}\")\n",
        "        print(f\"📄 Page: {doc.metadata.get('page', 'unknown')}\")\n",
        "        print(f\"🏷️ Rerank Score: {doc.metadata.get('rerank_score', 'N/A')}\")\n",
        "        #print(\"📝 Excerpt:\")\n",
        "        #print(doc.page_content.strip()[:500] + (\"...\" if len(doc.page_content) > 500 else \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_rag():\n",
        "    docs = ['https://assets.pokemon.com/assets/cms2/pdf/trading-card-game/rulebook/sm7_rulebook_en.pdf','https://media.wizards.com/images/magic/tcg/resources/rules/MagicCompRules_21031101.pdf','https://cdn.1j1ju.com/medias/d3/22/83-monopoly-rulebook.pdf','https://fgbradleys.com/wp-content/uploads/rules/Monopoly_Rules.pdf?srsltid=AfmBOorDaiGKyaEWIQFd-au0rl8-tKoqedlzy_6r4EETpj_ZMIUYsNMQ']\n",
        "    process_and_store_documents(docs,'800','80', persist_directory=\"./faiss_index\")\n",
        "    faiss_db = FAISS.load_local(\"./faiss_index\", EMBEDDING_MODEL, allow_dangerous_deserialization=True)\n",
        "    load_dotenv()\n",
        "    GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "    if not GEMINI_API_KEY:\n",
        "        raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
        "\n",
        "    llm = GoogleGenerativeAI(\n",
        "        api_key=GEMINI_API_KEY,\n",
        "        model=\"gemini-2.0-flash\",\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return faiss_db, llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prepare documents and vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Prepare documents\n",
        "# docs = ['https://assets.pokemon.com/assets/cms2/pdf/trading-card-game/rulebook/sm7_rulebook_en.pdf','https://media.wizards.com/images/magic/tcg/resources/rules/MagicCompRules_21031101.pdf','https://cdn.1j1ju.com/medias/d3/22/83-monopoly-rulebook.pdf','https://fgbradleys.com/wp-content/uploads/rules/Monopoly_Rules.pdf?srsltid=AfmBOorDaiGKyaEWIQFd-au0rl8-tKoqedlzy_6r4EETpj_ZMIUYsNMQ']\n",
        "# process_and_store_documents(docs,'800','80', persist_directory=\"./faiss_index\")\n",
        "# faiss_db = FAISS.load_local(\"./faiss_index\", EMBEDDING_MODEL, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rag_chain, rag_memory = create_rag_chain(\n",
        "#     vectorstore=faiss_db,\n",
        "#     llm=llm,\n",
        "#     prompt_type=\"react\",\n",
        "#     use_memory=True,\n",
        "#     retriever_k=3,\n",
        "#     use_reranking=True,\n",
        "#     top_k_chunks=10\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Testing with Sample Queries\n",
        "Let's test our RAG system with some sample queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# faiss_memory = create_memory_vectorstore(\n",
        "#     embedding_model=EMBEDDING_MODEL, path=\"memory_vectorstore\"\n",
        "# )\n",
        "\n",
        "# # Test some queries\n",
        "# test_queries = [\n",
        "#     \"what is EX card?\",\n",
        "#     \"what is haste?\",\n",
        "#     \"how to build a hotel?\"\n",
        "# ]\n",
        "\n",
        "# for query in test_queries:\n",
        "#     print(f\"\\n🔍 Query: {query}\")\n",
        "#     result = rag_chain({\"question\": query})\n",
        "#     print_result_summary(result)\n",
        "#     store_to_memory_vectorstore(\n",
        "#         question=query,\n",
        "#         answer=result[\"answer\"],\n",
        "#         vectorstore=faiss_memory\n",
        "#     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewritten prompt\n",
        "Here we will use LLM to rewrite prompts itself for better answers - TO BE IMPLEMENTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test_queries = [\n",
        "#     \"what is EX card?\",\n",
        "#     \"what is haste?\",\n",
        "#     \"how to build a hotel?\"\n",
        "# ]\n",
        "# for query in test_queries:\n",
        "#     result = rag_chain({\"question\": query, \"chat_history\": []})\n",
        "#     print_result_summary(result)\n",
        "#     store_to_memory_vectorstore(\n",
        "#         question=query,\n",
        "#         answer=result[\"answer\"],\n",
        "#         vectorstore=faiss_memory,         \n",
        "#         embedding_model=EMBEDDING_MODEL\n",
        "#     )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAGAS EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
        "\n",
        "# metrics = [faithfulness, answer_relevancy, context_precision]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from ragas import evaluate\n",
        "# from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "# # Wrap the LLM\n",
        "# evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "# # Evaluate the dataset\n",
        "# results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm)\n",
        "\n",
        "# # Display the results\n",
        "# print(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
