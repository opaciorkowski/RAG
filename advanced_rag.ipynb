{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG System with Gemini and FAISS\n",
        "# ================================"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Union\n",
        "\n",
        "import requests\n",
        "import faiss\n",
        "from dotenv import load_dotenv\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema import Document, BaseRetriever\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.docstore.in_memory import InMemoryDocstore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration\n",
        "First, let's set up the configuration for our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "\n",
        "# Configuration settings\n",
        "EMBEDDING = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "EMBEDDING_MODEL = HuggingFaceEmbeddings(model_name=EMBEDDING)\n",
        "CHUNK_SIZE = 800\n",
        "CHUNK_OVERLAP = 80\n",
        "RETRIEVER_K = 4\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not GEMINI_API_KEY:\n",
        "    raise ValueError(\"GEMINI_API_KEY not found in environment variables\")\n",
        "\n",
        "llm = GoogleGenerativeAI(\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Document Processing Functions\n",
        "Let's define functions to download and process PDF documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_pdf(url: str, folder: str = 'documents') -> str:\n",
        "    \"\"\"\n",
        "    Downloads PDF from given URL\n",
        "    \"\"\"\n",
        "    os.makedirs(folder, exist_ok=True)\n",
        "    filename = os.path.basename(url.split('?')[0])\n",
        "    filepath = os.path.join(folder, filename)\n",
        "\n",
        "    response = requests.get(url, timeout=10)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(filepath, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    return filepath\n",
        "\n",
        "def process_document(\n",
        "    documents: Union[List[str], List[Document]], chunk_size: int, chunk_overlap: int) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Process a list of Document objects or URLs into chunks while preserving parent document relationships.\n",
        "    \"\"\"\n",
        "    if isinstance(chunk_size, str):\n",
        "        chunk_size = int(chunk_size)\n",
        "    if isinstance(chunk_overlap, str):\n",
        "        chunk_overlap = int(chunk_overlap)\n",
        "\n",
        "    if documents and isinstance(documents[0], str):\n",
        "        loaded_docs = []\n",
        "        for url in documents:\n",
        "            pdf_path = download_pdf(url)\n",
        "            pdf_docs = PyPDFLoader(pdf_path).load()\n",
        "            loaded_docs.extend(pdf_docs)\n",
        "        documents = loaded_docs\n",
        "\n",
        "    def get_filename(path):\n",
        "        if not path or path == \"unknown\":\n",
        "            return \"unknown_document\"\n",
        "        return os.path.basename(path).split('.')[0]\n",
        "\n",
        "    source_groups = {}\n",
        "    for doc in documents:\n",
        "        source = doc.metadata.get(\"source\", \"unknown\")\n",
        "        source_groups.setdefault(source, []).append(doc)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "\n",
        "    all_chunks = []\n",
        "    for source, docs in source_groups.items():\n",
        "        docs.sort(key=lambda x: x.metadata.get(\"page\", 0))\n",
        "        parent_id = get_filename(source)\n",
        "        chunks = splitter.split_documents(docs)\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            page_num = chunk.metadata.get(\"page\", 0)\n",
        "            chunk_id = f\"{parent_id}_p{page_num}_c{i}\"\n",
        "            chunk.metadata.update({\n",
        "                \"parent_id\": parent_id,\n",
        "                \"parent_source\": source,\n",
        "                \"chunk_id\": chunk_id,\n",
        "                \"chunk_index\": i,\n",
        "                \"total_chunks\": len(chunks)\n",
        "            })\n",
        "            all_chunks.append(chunk)\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Vector Store Creation\n",
        "Now let's create a function to build our vector store from documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_and_store_documents(documents: List[str], \n",
        "                              chunk_size: int, \n",
        "                              chunk_overlap: int,\n",
        "                              embedding_model = EMBEDDING_MODEL,\n",
        "                              persist_directory: str = None) -> FAISS:\n",
        "    \"\"\"\n",
        "    Process documents into chunks and store them in a FAISS vector database\n",
        "    while preserving parent document relationships.\n",
        "    \"\"\"\n",
        "    # First process the documents into chunks with parent metadata\n",
        "    chunks = process_document(documents, chunk_size, chunk_overlap)\n",
        "    \n",
        "    vector_store = FAISS.from_documents(chunks, embedding_model)\n",
        "    \n",
        "    if persist_directory:\n",
        "        vector_store.save_local(persist_directory)\n",
        "    \n",
        "    print(f\"Added {len(chunks)} chunks from {len(set([c.metadata['parent_id'] for c in chunks]))} parent documents to FAISS vector store\")\n",
        "    \n",
        "    return vector_store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Long-Term memory storage in Vectorstore\n",
        "This part is created to store conversation with LLM as additional context to be provided during retriving data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_memory_vectorstore(embedding_model, path=\"memory_vectorstore\"):\n",
        "    if Path(path).exists():\n",
        "        return FAISS.load_local(path, embedding_model, allow_dangerous_deserialization=True)\n",
        "    \n",
        "    dim = len(embedding_model.embed_query(\"test\"))\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "\n",
        "    docstore = InMemoryDocstore()\n",
        "    index_to_docstore_id = {}\n",
        "    \n",
        "    return FAISS(\n",
        "        embedding_function=embedding_model,\n",
        "        index=index,\n",
        "        docstore=docstore,\n",
        "        index_to_docstore_id=index_to_docstore_id\n",
        "    )\n",
        "\n",
        "def store_to_memory_vectorstore(question, answer, vectorstore):\n",
        "    content = f\"Q: {question}\\nA: {answer}\"\n",
        "    doc = Document(page_content=content, metadata={\"type\": \"chat_memory\"})\n",
        "    vectorstore.add_documents([doc])\n",
        "    vectorstore.save_local(\"memory_vectorstore\")\n",
        "    \n",
        "def get_relevant_memory(query, vectorstore, k=3):\n",
        "    try:\n",
        "        memory_docs = vectorstore.similarity_search(query, k=k)\n",
        "        return \"\\n---\\n\".join([doc.page_content for doc in memory_docs])\n",
        "    except (IndexError, ValueError) as e:\n",
        "        print(f\"Skipping memory retrieval (reason: {str(e)})\")\n",
        "        return \"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Reranker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_parent_document_llm_reranker(vectorstore, top_k_chunks=20, top_k_parents=4):\n",
        "    class LLMRerankerRetriever(BaseRetriever):\n",
        "        def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "            cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
        "\n",
        "            relevant_chunks_with_scores = vectorstore.similarity_search_with_score(query, k=top_k_chunks)\n",
        "            chunks = [doc for doc, _ in relevant_chunks_with_scores]\n",
        "            scores = [score for _, score in relevant_chunks_with_scores]\n",
        "\n",
        "            parent_docs = {}\n",
        "            for chunk, score in zip(chunks, scores):\n",
        "                parent_id = chunk.metadata.get(\"parent_id\") or chunk.metadata.get(\"doc_id\", f\"doc_{len(parent_docs)}\")\n",
        "                if parent_id not in parent_docs:\n",
        "                    parent_docs[parent_id] = {\n",
        "                        \"chunks\": [],\n",
        "                        \"scores\": [],\n",
        "                        \"source\": chunk.metadata.get(\"parent_source\", \"unknown\")\n",
        "                    }\n",
        "                parent_docs[parent_id][\"chunks\"].append(chunk)\n",
        "                parent_docs[parent_id][\"scores\"].append(score)\n",
        "\n",
        "            parent_list = []\n",
        "            for parent_id, parent in parent_docs.items():\n",
        "                parent[\"chunks\"].sort(key=lambda x: (x.metadata.get(\"page\", 0), x.metadata.get(\"chunk_index\", 0)))\n",
        "                full_text = \"\\n\".join([chunk.page_content for chunk in parent[\"chunks\"]])\n",
        "\n",
        "                try:\n",
        "                    rerank_score = float(cross_encoder.predict([(query, full_text)])[0])\n",
        "                except Exception:\n",
        "                    rerank_score = 0.0\n",
        "\n",
        "                for c in parent[\"chunks\"]:\n",
        "                    c.metadata[\"rerank_score\"] = rerank_score\n",
        "\n",
        "                parent_list.append({\n",
        "                    \"id\": parent_id,\n",
        "                    \"chunks\": parent[\"chunks\"],\n",
        "                    \"rerank_score\": rerank_score,\n",
        "                    \"source\": parent[\"source\"]\n",
        "                })\n",
        "\n",
        "            parent_list.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
        "\n",
        "            top_docs = []\n",
        "            for parent in parent_list[:top_k_parents]:\n",
        "                top_docs.extend(parent[\"chunks\"])\n",
        "\n",
        "            return top_docs\n",
        "\n",
        "        async def _aget_relevant_documents(self, query: str):\n",
        "            raise NotImplementedError(\"Async version not implemented.\")\n",
        "\n",
        "    return LLMRerankerRetriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Different types of prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "def get_prompt(prompt_type: str, role: str = \"Ai Assistant\") -> PromptTemplate:\n",
        "    if prompt_type == \"zero_shot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Be clear and concise.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"explain_like_5\":\n",
        "        template = \"\"\"Use the following pieces of context to answer the question. Explain like you are talking to a 5-year-old. \n",
        "        If the question is not related to the context, say \"I don't know\". If you don't know the answer, just say that you don't know.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Provide a clear and concise answer.\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"cot\":\n",
        "        template = \"\"\"Use the following context to answer the question. Think step-by-step and explain your reasoning.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Let's think step by step:\n",
        "\n",
        "        Answer:\"\"\"\n",
        "    elif prompt_type == \"elaborate\":\n",
        "        template = \"\"\"Use the following context to answer the question in a detailed, formal tone. If you can't answer, say \"I don't know\".\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Detailed answer:\"\"\"\n",
        "    elif prompt_type == \"meta\":\n",
        "        template = \"\"\"You are an AI assistant tasked with answering the question using the provided context. \n",
        "        First, generate an optimal prompt that would help an LLM perform this task effectively.\n",
        "        Then, respond to that prompt yourself to complete the task.\n",
        "        Reflect on your reasoning process as you answer. Clearly state what you know, what you are assuming, and how confident you are.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer (include reasoning, assumptions, and confidence level):\"\"\"\n",
        "    elif prompt_type == \"role\":\n",
        "        template = \"\"\"You are acting as {role}. Use the following context to answer the question appropriately for your role.\n",
        "        If the question is not related to the context, say \"I don't know\". If you're unsure of the answer, acknowledge that.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        As a {role}, your answer:\"\"\"\n",
        "    elif prompt_type == \"react\":\n",
        "        template = \"\"\"You are an intelligent assistant that reasons step-by-step and can use external context to answer questions.\n",
        "\n",
        "        Use the following format:\n",
        "\n",
        "        Question: {question}\n",
        "        Thought: Think about what you need to find.\n",
        "        Action: Look up relevant information in the context.\n",
        "        Observation: Summarize what the context says.\n",
        "        Thought: Reflect on how the observation answers the question.\n",
        "        Final Answer: Give a complete and clear answer.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Now follow the steps to answer:\n",
        "        \"\"\"\n",
        "    elif prompt_type == \"verify\":\n",
        "        template = \"\"\"\n",
        "        Given the context and the answer, verify if the answer is fully supported. \n",
        "        Respond with YES or NO, then explain briefly.\n",
        "        \n",
        "        Context:\n",
        "        {context}\n",
        "        \n",
        "        Answer:\n",
        "        {answer}\n",
        "        \n",
        "        Is this answer verifiable?\n",
        "        \"\"\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown prompt type: {prompt_type}\")\n",
        "\n",
        "    return PromptTemplate.from_template(template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. RAG Chain Creation\n",
        "Let's create our RAG chain with the Gemini model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_rag_chain(\n",
        "    vectorstore,\n",
        "    llm,\n",
        "    prompt_type: str = \"zero_shot\",\n",
        "    use_memory: bool = False,\n",
        "    use_reranking: bool = False,\n",
        "    retriever_k: int = 4,\n",
        "    top_k_chunks: int = 20\n",
        "):\n",
        "    \"\"\"\n",
        "    Create a RAG chain with optional memory, reranking, and query rewriting.\n",
        "    \"\"\"\n",
        "    prompt = get_prompt(prompt_type)\n",
        "\n",
        "    memory = (\n",
        "        ConversationBufferMemory(\n",
        "            memory_key=\"chat_history\",\n",
        "            input_key=\"question\",\n",
        "            output_key=\"answer\",\n",
        "            return_messages=True\n",
        "        )\n",
        "        if use_memory else None\n",
        "    )\n",
        "\n",
        "    if use_reranking:\n",
        "        retriever = create_parent_document_llm_reranker(\n",
        "            vectorstore=vectorstore,\n",
        "            top_k_chunks=top_k_chunks,\n",
        "            top_k_parents=retriever_k\n",
        "        )\n",
        "    else:\n",
        "        retriever = vectorstore.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": retriever_k}\n",
        "        )\n",
        "\n",
        "    base_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=retriever,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt},\n",
        "        return_source_documents=True,\n",
        "        memory=memory,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    return base_chain, memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_result_summary(result):\n",
        "    print(f\"\\n🧠 Answer:\\n{result['answer']}\\n\")\n",
        "\n",
        "    print(\"🔎 Retrieved Documents:\")\n",
        "    for i, doc in enumerate(result[\"source_documents\"], 1):\n",
        "        print(f\"\\n📄 Chunk #{i}\")\n",
        "        print(f\"📚 Source: {doc.metadata.get('parent_source', 'unknown')}\")\n",
        "        print(f\"📄 Page: {doc.metadata.get('page', 'unknown')}\")\n",
        "        print(f\"🏷️ Rerank Score: {doc.metadata.get('rerank_score', 'N/A')}\")\n",
        "        #print(\"📝 Excerpt:\")\n",
        "        #print(doc.page_content.strip()[:500] + (\"...\" if len(doc.page_content) > 500 else \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Prepare documents and vectorstore\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 1074 chunks from 4 parent documents to FAISS vector store\n"
          ]
        }
      ],
      "source": [
        "# Prepare documents\n",
        "docs = ['https://assets.pokemon.com/assets/cms2/pdf/trading-card-game/rulebook/sm7_rulebook_en.pdf','https://media.wizards.com/images/magic/tcg/resources/rules/MagicCompRules_21031101.pdf','https://cdn.1j1ju.com/medias/d3/22/83-monopoly-rulebook.pdf','https://fgbradleys.com/wp-content/uploads/rules/Monopoly_Rules.pdf?srsltid=AfmBOorDaiGKyaEWIQFd-au0rl8-tKoqedlzy_6r4EETpj_ZMIUYsNMQ']\n",
        "process_and_store_documents(docs,'800','80', persist_directory=\"./faiss_index\")\n",
        "faiss_db = FAISS.load_local(\"./faiss_index\", EMBEDDING_MODEL, allow_dangerous_deserialization=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "rag_chain, rag_memory = create_rag_chain(\n",
        "    vectorstore=faiss_db,\n",
        "    llm=llm,\n",
        "    prompt_type=\"react\",\n",
        "    use_memory=False,\n",
        "    retriever_k=3,\n",
        "    use_reranking=True,\n",
        "    top_k_chunks=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Testing with Sample Queries\n",
        "Let's test our RAG system with some sample queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔍 Query: what is EX card?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are trying to use a model that was created with Sentence Transformers version 4.1.0.dev0, but you're currently using version 4.0.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Answer:\n",
            "Question: what is EX card?\n",
            "Thought: I need to find the definition and special rules of Pokemon-EX cards from the provided text.\n",
            "Action: I will search the text for information about \"Pokémon-EX\".\n",
            "Observation: The text defines Pokémon-EX as powerful Pokémon with more HP and stronger attacks. It highlights that the \"EX\" is part of the name, affecting deck limits. A key drawback is that when a Pokémon-EX is Knocked Out, the opponent takes 2 Prize cards. Otherwise, they play like regular Pokémon cards and can have regular and full-art versions.\n",
            "Thought: The observation gives a comprehensive answer to the question.\n",
            "Final Answer: Pokémon-EX are powerful Pokémon cards with higher HP and stronger attacks than regular Pokémon. The \"EX\" is part of the name, allowing up to 4 of each version (e.g., Yveltal and Yveltal-EX) in a deck. The downside is that when a Pokémon-EX is Knocked Out, the opponent takes 2 Prize cards. They otherwise play like regular Pokémon cards and can have regular and full-art versions.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 27\n",
            "🏷️ Rerank Score: 4.7002272605896\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\sm7_rulebook_en.pdf\n",
            "📄 Page: 32\n",
            "🏷️ Rerank Score: 4.7002272605896\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 14\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #4\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 14\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #5\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 38\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #6\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 94\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #7\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 94\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #8\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 168\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #9\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 175\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "📄 Chunk #10\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 185\n",
            "🏷️ Rerank Score: -5.002190589904785\n",
            "\n",
            "🔍 Query: what is haste?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are trying to use a model that was created with Sentence Transformers version 4.1.0.dev0, but you're currently using version 4.0.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Answer:\n",
            "Question: what is haste?\n",
            "Thought: I need to find the definition of haste in the context provided.\n",
            "Action: I will look for sections that define or explain the term \"haste.\"\n",
            "Observation: The context includes the following information about haste:\n",
            "\n",
            "*   702.10. Haste\n",
            "*   702.10a Haste is a static ability.\n",
            "*   702.10b If a creature has haste, it can attack even if it hasn’t been controlled by its controller continuously since his or her most recent turn began. (See rule 302.6.)\n",
            "*   702.10c If a creature has haste, its controller can activate its activated abilities whose cost includes the tap symbol or the untap symbol even if that creature hasn’t been controlled by that player continuously since his or her most recent turn began. (See rule 302.6.)\n",
            "*   702.10d Multiple instances of haste on the same creature are redundant.\n",
            "\n",
            "Thought: I can use the information above to create a concise definition.\n",
            "Final Answer: Haste is a static ability that allows a creature to attack and use tap or untap abilities without having been continuously controlled by its controller since the beginning of their most recent turn. Multiple instances of haste on the same creature are redundant.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 23\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 70\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 84\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #4\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 102\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #5\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 102\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #6\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 116\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #7\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 126\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #8\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 185\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #9\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 188\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "📄 Chunk #10\n",
            "📚 Source: documents\\MagicCompRules_21031101.pdf\n",
            "📄 Page: 192\n",
            "🏷️ Rerank Score: -4.312689781188965\n",
            "\n",
            "🔍 Query: how to build a hotel?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are trying to use a model that was created with Sentence Transformers version 4.1.0.dev0, but you're currently using version 4.0.2. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧠 Answer:\n",
            "Question: how to build a hotel?\n",
            "Thought: I need to find the rules for building a hotel in the context.\n",
            "Action: I will search for the section on hotels.\n",
            "Observation: The context states: \"When a player has four houses on each property of a complete color-group, he may buy a hotel from the Bank and erect it on any property of that color-group. He returns the four houses from that property to the Bank and pays the price for the hotel as shown on the Title Deed card. Only one hotel may be erected on any one property.\"\n",
            "Thought: The observation describes how to build a hotel.\n",
            "Final Answer: To build a hotel, you must first have four houses on each property of a complete color-group. Then, you can buy a hotel from the Bank and place it on any property within that color-group. You must return the four houses from that property to the Bank and pay the price for the hotel as indicated on the Title Deed card. Only one hotel is allowed on any single property.\n",
            "\n",
            "🔎 Retrieved Documents:\n",
            "\n",
            "📄 Chunk #1\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 0\n",
            "🏷️ Rerank Score: -1.470438003540039\n",
            "\n",
            "📄 Chunk #2\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 2\n",
            "🏷️ Rerank Score: -1.470438003540039\n",
            "\n",
            "📄 Chunk #3\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 2\n",
            "🏷️ Rerank Score: -1.470438003540039\n",
            "\n",
            "📄 Chunk #4\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 2\n",
            "🏷️ Rerank Score: -1.470438003540039\n",
            "\n",
            "📄 Chunk #5\n",
            "📚 Source: documents\\Monopoly_Rules.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: -1.470438003540039\n",
            "\n",
            "📄 Chunk #6\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 0\n",
            "🏷️ Rerank Score: -2.0440268516540527\n",
            "\n",
            "📄 Chunk #7\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: -2.0440268516540527\n",
            "\n",
            "📄 Chunk #8\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: -2.0440268516540527\n",
            "\n",
            "📄 Chunk #9\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 3\n",
            "🏷️ Rerank Score: -2.0440268516540527\n",
            "\n",
            "📄 Chunk #10\n",
            "📚 Source: documents\\83-monopoly-rulebook.pdf\n",
            "📄 Page: 4\n",
            "🏷️ Rerank Score: -2.0440268516540527\n"
          ]
        }
      ],
      "source": [
        "faiss_memory = create_memory_vectorstore(\n",
        "    embedding_model=EMBEDDING_MODEL, path=\"memory_vectorstore\"\n",
        ")\n",
        "\n",
        "# Test some queries\n",
        "test_queries = [\n",
        "    \"what is EX card?\",\n",
        "    \"what is haste?\",\n",
        "    \"how to build a hotel?\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n🔍 Query: {query}\")\n",
        "    result = rag_chain({\"question\": query, \"chat_history\": []})\n",
        "    print_result_summary(result)\n",
        "    store_to_memory_vectorstore(\n",
        "        question=query,\n",
        "        answer=result[\"answer\"],\n",
        "        vectorstore=faiss_memory\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewritten prompt\n",
        "Here we will use LLM to rewrite prompts itself for better answers - TO BE IMPLEMENTED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_queries = [\n",
        "    \"what is EX card?\",\n",
        "    \"what is haste?\",\n",
        "    \"how to build a hotel?\"\n",
        "]\n",
        "for query in test_queries:\n",
        "    result = rag_chain({\"question\": query, \"chat_history\": []})\n",
        "    print_result_summary(result)\n",
        "    store_to_memory_vectorstore(\n",
        "        question=query,\n",
        "        answer=result[\"answer\"],\n",
        "        vectorstore=faiss_memory,         \n",
        "        embedding_model=EMBEDDING_MODEL\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAGAS EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Example evaluation data\n",
        "eval_data = [\n",
        "    {\n",
        "        \"question\": \"What is a Pokémon EX card?\",\n",
        "        \"answer\": \"A Pokémon-EX card is a powerful type of card with higher HP and stronger attacks. When it’s knocked out, the opponent takes 2 Prize cards.\",\n",
        "        \"contexts\": [\n",
        "            \"Pokémon-EX cards have more HP and stronger attacks than regular Pokémon cards. If one is knocked out, the opponent takes 2 Prize cards.\"\n",
        "        ],\n",
        "        \"reference\": \"Pokémon-EX cards are a special kind of card that have more HP and stronger attacks than normal cards. When knocked out, they give the opponent 2 Prize cards.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What does haste do in Magic: The Gathering?\",\n",
        "        \"answer\": \"Haste allows creatures to attack and use abilities immediately after being played, ignoring summoning sickness.\",\n",
        "        \"contexts\": [\n",
        "            \"Creatures with haste can attack or use tap/untap abilities even if they haven't been under the player's control since the beginning of the turn.\"\n",
        "        ],\n",
        "        \"reference\": \"Haste is a keyword that allows a creature to attack or use abilities as soon as it enters the battlefield, bypassing summoning sickness.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How can a player build a hotel in Monopoly?\",\n",
        "        \"answer\": \"To build a hotel, a player must first own all properties in a color group and build four houses on each property. Then, they can exchange the houses for a hotel.\",\n",
        "        \"contexts\": [\n",
        "            \"Once a player has four houses on each property in a color group, they may buy a hotel and place it on any of those properties. The four houses are returned to the bank.\"\n",
        "        ],\n",
        "        \"reference\": \"A player must have four houses on every property in a color set to buy a hotel. The player exchanges the houses for one hotel and pays the bank the cost listed on the title deed.\"\n",
        "    }\n",
        "]\n",
        "\n",
        "eval_dataset = Dataset.from_list(eval_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
        "\n",
        "metrics = [faithfulness, answer_relevancy, context_precision]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "\n",
        "# Wrap the LLM\n",
        "evaluator_llm = LangchainLLMWrapper(llm)\n",
        "\n",
        "# Evaluate the dataset\n",
        "results = evaluate(dataset=eval_dataset, metrics=metrics, llm=evaluator_llm)\n",
        "\n",
        "# Display the results\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
